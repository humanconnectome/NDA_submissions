{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import funcs\n",
    "from Crosswalk.Transformer import Transformer\n",
    "from Crosswalk.DataCache import DataCache\n",
    "from Crosswalk.NDAWriter import NDAWriter\n",
    "from Crosswalk.Manager import Manager\n",
    "\n",
    "from Crosswalk.Loader import Loader, BoxLoader, BoxHcaLoader, SsagaLoader, QintHcaLoader, RedcapLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folder for prepped structures, if it doesn't exist\n",
    "!!mkdir prepped\n",
    "!!mkdir prepped/hca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the path to the validator below.  This is the vtcmd.py validator written by the NDA.  \n",
    "# If you haven't already installed this, please do so now: https://github.com/NDAR/nda-tools\n",
    "# and the place the path that shows up when you type 'which vtcmd' from your terminal\n",
    "# validation results will be sent to and read from whatever default is specified in the vtcmd configuration file,\n",
    "# so if you're using vtcmd to validate any other datatypes, keep this in mind.\n",
    "\n",
    "M = Manager(\n",
    "        data =  DataCache(\n",
    "            BoxHcaLoader('PennCNP',592325063896),\n",
    "            RedcapLoader('hcpa'),\n",
    "#             SsagaLoader(),\n",
    "            QintHcaLoader()\n",
    "        ),\n",
    "        writer = NDAWriter(completed_dir=\"./prepped/hca/\", validator=\"/home/petra/.local/bin/vtcmd\"),\n",
    "        #writer = NDAWriter(completed_dir=\"./prepped/hca/\"),\n",
    "        transformer = Transformer(funcs = funcs, map_dir='./maps/hca/')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step requires that you have a 'rosetta stone' file that has all the required NDA fields for \n",
    "# all subjects you intend to submit at this time.  This approach facilitates keeping track of subject counts\n",
    "# across data types.  For example, if your required fields are already stored in XNAT because you had the CCF\n",
    "# upload your imaging data for you, you can export this csv from XNAT and rename as appropriate.  \n",
    "# Place this file at the main level of this repository, and name it in your config file\n",
    "# Loader.py program's _post_load_hook_ method referenced below.  \n",
    "\n",
    "M.preload_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ad hoc functions to clean up empty rows for particular instruments after generated (issue for redcap data)\n",
    "def redcleanup(structure=\"lbadl01\",filePath=\"./prepped/hca/\",extraomitcol1='NO',extraomitcol2='NO',extraomitcol3='NO',extraomitcol4='NO'):\n",
    "    print(structure)\n",
    "    strucroot=structure[:-2]\n",
    "    strucnum=structure[-2:]\n",
    "\n",
    "    df=pd.read_csv(filePath+structure+\".csv\",header=1)\n",
    "    df.head()\n",
    "\n",
    "    print(\"NumRows Before: \"+str(df.shape[0]))\n",
    "    subfields=df.columns.to_list()\n",
    "    subfields.remove('subjectkey')\n",
    "    subfields.remove('src_subject_id')\n",
    "    subfields.remove('interview_date')\n",
    "    subfields.remove('interview_age')\n",
    "    subfields.remove('sex')\n",
    "    if extraomitcol1 and extraomitcol1 !='NO':\n",
    "        subfields.remove(extraomitcol1)\n",
    "    if extraomitcol2 and extraomitcol2 !='NO':\n",
    "        subfields.remove(extraomitcol2)\n",
    "    if extraomitcol3 and extraomitcol3 !='NO':\n",
    "        subfields.remove(extraomitcol3)\n",
    "    if extraomitcol4 and extraomitcol4 !='NO':\n",
    "        subfields.remove(extraomitcol4)\n",
    "    df=df.dropna(how='all',subset=subfields)\n",
    "    print(\"NumRows After: \"+str(df.shape[0]))\n",
    "\n",
    "    with open(filePath+structure+\".csv\",'w') as f:\n",
    "        f.write(strucroot+\",\"+str(int(strucnum))+\"\\n\")\n",
    "        df.to_csv(f,index=False)\n",
    " \n",
    "\n",
    "#these guys already set to 99s in map, so null finder wont work above\n",
    "def asrover60(structure=\"asr01\",filePath=\"./prepped/hca/\"):\n",
    "    print(structure)\n",
    "    strucroot=structure[:-2]\n",
    "    strucnum=structure[-2:]\n",
    "    df=pd.read_csv(filePath+structure+\".csv\",header=1)\n",
    "    print(\"NumRows Before: \"+str(df.shape[0]))\n",
    "    #df=df.loc[df.interview_age>719].copy()\n",
    "    df=df.loc[~((df.asr2_2==-99)&(df.asr3_2==-99))]\n",
    "    df=df.drop(columns=['asr2_3_text',\n",
    "        'oasr_ppl9_des',\n",
    "        'asr5_5_text',\n",
    "        'asr7_4_text',\n",
    "        'asr8_4_text',\n",
    "        'asr10_6_text',\n",
    "        'asr13_3_text',\n",
    "        'asr14_1_text',\n",
    "        'asr15_2_text',\n",
    "        'asr16_3_text',\n",
    "        'asr16_4_text',\n",
    "        'asr17_5_text',\n",
    "        'asr19_1_text',\n",
    "        'cbcl56h_des'\n",
    "        ])\n",
    "    print(\"NumRows After: \"+str(df.shape[0]))\n",
    "    with open(filePath+structure+\".csv\",'w') as f:\n",
    "        f.write(strucroot+\",\"+str(int(strucnum))+\"\\n\")\n",
    "        df.to_csv(f,index=False)\n",
    "    #print(subset)\n",
    "        \n",
    "def satisfy(structure='scan_debrief01',filePath=\"./prepped/hca/\"):  \n",
    "    print(structure)\n",
    "    strucroot=structure[:-2]\n",
    "    strucnum=structure[-2:]\n",
    "    df=pd.read_csv(filePath+structure+\".csv\",header=1)\n",
    "    print(\"NumRows Before: \"+str(df.shape[0]))\n",
    "    df=df.drop(columns=['satisfaction1more','satisfaction2more','satisfaction4more','satisfaction5','satisfaction6'])\n",
    "    print(\"NumRows After: \"+str(df.shape[0]))\n",
    "    with open(filePath+structure+\".csv\",'w') as f:\n",
    "        f.write(strucroot+\",\"+str(int(strucnum))+\"\\n\")\n",
    "        df.to_csv(f,index=False)\n",
    "\n",
    "        \n",
    "def cleanlist(structurelist=['lbadl01','mchq01']):\n",
    "    for i in structurelist:\n",
    "        print(i)\n",
    "        redcleanup(structure=i,filePath=\"./prepped/hca/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.run('scan_debrief01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The commented out structs dont seem to be laid out as requested...need to investigate further...perhaps turn into \n",
    "#singleton structure, since doesnt follow typical one row per subject format\n",
    "structs = [\n",
    "    'lbadl01',\n",
    "    'mchq01',\n",
    "    'er4001',\n",
    "    'deldisk01',\n",
    "    'asr01',\n",
    "    'batbil01',\n",
    "    'bsc01',\n",
    "    'drugscr01',\n",
    "    'gales01',\n",
    "    'ipaq01',\n",
    "    'leap01',\n",
    "    'medh01',\n",
    "    'mendt01',\n",
    "    'moca01',\n",
    "    'nffi01',\n",
    "    'psqi01',\n",
    "    'ravlt01',\n",
    "    'scan_debrief01',\n",
    "  ]\n",
    "\n",
    "for s in structs:\n",
    "    M.run(s)\n",
    "    print(s)   \n",
    "cleanlist(structurelist=structs)\n",
    "asrover60(structure=\"asr01\",filePath=\"./prepped/hca/\")\n",
    "redcleanup(structure=\"deldisk01\",filePath=\"./prepped/hca/\",extraomitcol1='version_form',extraomitcol2='comqother')\n",
    "redcleanup(structure=\"medh01\",filePath=\"./prepped/hca/\",extraomitcol1='comqother')\n",
    "redcleanup(structure=\"bsc01\",filePath=\"./prepped/hca/\",extraomitcol1='comqother')\n",
    "redcleanup(structure=\"ravlt01\",filePath=\"./prepped/hca/\",extraomitcol1='ravlt_delt',extraomitcol2='ravlt_disct',extraomitcol3='ravlt_tott')\n",
    "redcleanup(structure=\"scan_debrief01\",filePath=\"./prepped/hca/\",extraomitcol1='comqother')\n",
    "satisfy(structure='scan_debrief01',filePath=\"./prepped/hca/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSAGA and not mapped at this time because NDA wanted to reconsider how they organize this info per 5/1/20 email.  \n",
    "structs2 = [\n",
    "#     'diagpsx01',\n",
    "#     'eatdisdemo01',\n",
    "#     'phenx_sib01',\n",
    "#     'scidv_pscyh01',\n",
    "#     'socdem01'\n",
    "  ]\n",
    "\n",
    "for s in structs2:\n",
    "    M.run(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NDA_submissions_venv",
   "language": "python",
   "name": "nda_submissions_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
