{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import funcs\n",
    "from Crosswalk.Transformer import Transformer\n",
    "from Crosswalk.DataCache import DataCache\n",
    "from Crosswalk.NDAWriter import NDAWriter\n",
    "from Crosswalk.Manager import Manager\n",
    "\n",
    "from Crosswalk.Loader import BoxLoader, BoxHcdLoader, QintHcdLoader, RedcapLoader, ParentLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ccf.easy_yaml import EasyYaml\n",
    "from ccf.redcap import RedcapTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folder for prepped structures, if it doesn't exist\n",
    "!!mkdir prepped\n",
    "!!mkdir prepped/hcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = Manager(\n",
    "        data =  DataCache(\n",
    "            ParentLoader(),\n",
    "            RedcapLoader('child'),\n",
    "            RedcapLoader('teen'),\n",
    "            BoxHcdLoader('PennCNP', 592325063896),\n",
    "            QintHcdLoader()\n",
    "        ),\n",
    "        writer = NDAWriter(completed_dir=\"./prepped/hcd/\", validator=\"/home/petra/.local/bin/vtcmd\"),\n",
    "        transformer = Transformer(funcs = funcs, map_dir='./maps/hcd/')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step requires that you have a 'rosetta stone' file that has all the required NDA fields for \n",
    "# all subjects you intend to submit at this time.  This approach facilitates keeping track of subject counts\n",
    "# across data types.  For example, if your required fields are already stored in XNAT because you had the CCF\n",
    "# upload your imaging data for you, you can export this csv from XNAT and rename as appropriate.  \n",
    "# Place this file at the main level of this repository, and name it in your config file\n",
    "# Loader.py program's _post_load_hook_ method referenced below.  the method is currently hardcoded to read this csv and rename \n",
    "# columns to NDA requirements of ['subject', 'subjectkey', 'gender', 'interview_date', 'interview_age']\n",
    "# as follows.  \n",
    "        #rosetta = pd.read_csv('UnrelatedHCAHCD_w_STG_Image_and_pseudo_GUID05_27_2020.csv')\n",
    "        #rosetta = rosetta[['subjectped', 'nda_guid', 'nda_gender', 'nda_interview_date', 'nda_interview_age']]\n",
    "        #rosetta.columns = ['subject', 'subjectkey', 'gender', 'interview_date', 'interview_age']\n",
    "#future versions of this code will pull out this file into config.py or even better place, if demand warrants.\n",
    "#For now, just tweak this function to read your own rosetta file, making sure to result in csv with required, or\n",
    "# fill out the template file and save it as 'UnrelatedHCAHCD_w_STG_Image_and_pseudo_GUID05_27_2020.csv' or whatever \n",
    "# you want it to be under the 'rosetta' attribute in the config file\n",
    "\n",
    "M.preload_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ad hoc functions to clean up empty rows for particular instruments after generated (issue for redcap data)\n",
    "def redcleanup(structure=\"lbadl01\",filePath=\"./prepped/hcd/\",extraomitcol1='NO',extraomitcol2='NO',extraomitcol3='NO',extraomitcol4='NO'):\n",
    "    print(structure)\n",
    "    strucroot=structure[:-2]\n",
    "    strucnum=structure[-2:]\n",
    "\n",
    "    df=pd.read_csv(filePath+structure+\".csv\",header=1)\n",
    "    df.head()\n",
    "\n",
    "    print(\"NumRows Before: \"+str(df.shape[0]))\n",
    "    subfields=df.columns.to_list()\n",
    "    subfields.remove('subjectkey')\n",
    "    subfields.remove('src_subject_id')\n",
    "    subfields.remove('interview_date')\n",
    "    subfields.remove('interview_age')\n",
    "    subfields.remove('sex')\n",
    "    subfields.remove('comqother')\n",
    "    if extraomitcol1 and extraomitcol1 !='NO':\n",
    "        subfields.remove(extraomitcol1)\n",
    "    if extraomitcol2 and extraomitcol2 !='NO':\n",
    "        subfields.remove(extraomitcol2)\n",
    "    if extraomitcol3 and extraomitcol3 !='NO':\n",
    "        subfields.remove(extraomitcol3)\n",
    "    if extraomitcol4 and extraomitcol4 !='NO':\n",
    "        subfields.remove(extraomitcol4)\n",
    "    df=df.dropna(how='all',subset=subfields)\n",
    "    print(\"NumRows After: \"+str(df.shape[0]))\n",
    "\n",
    "    with open(filePath+structure+\".csv\",'w') as f:\n",
    "        f.write(strucroot+\",\"+str(int(strucnum))+\"\\n\")\n",
    "        df.to_csv(f,index=False)\n",
    "   \n",
    "#these guys already set to 99s in map, so null finder wont work above\n",
    "def asr(structure=\"asr01\",filePath=\"./prepped/hcd/\"):\n",
    "    print(structure)\n",
    "    strucroot=structure[:-2]\n",
    "    strucnum=structure[-2:]\n",
    "    df=pd.read_csv(filePath+structure+\".csv\",header=1)\n",
    "    print(\"NumRows Before: \"+str(df.shape[0]))\n",
    "    print(\"NumColumns Before: \"+str(df.shape[1]))\n",
    "    #df=df.loc[df.interview_age>719].copy()\n",
    "    df=df.loc[~((df.asr2_2==-99)&(df.asr3_2==-99))]\n",
    "    df=df.drop(columns=['asr1_6_text',\n",
    "        'asr2_3_text',\n",
    "        'asr5_5_text',\n",
    "        'asr7_4_text',\n",
    "        'asr8_4_text',\n",
    "        'asr10_6_text',\n",
    "        'asr12_1_text',                \n",
    "        'asr13_3_text',\n",
    "        'asr14_1_text',\n",
    "        'asr15_2_text',\n",
    "        'asr15_4_text',\n",
    "        'asr16_3_text',\n",
    "        'asr16_4_text',\n",
    "        'asr17_5_text',\n",
    "        'asr19_1_text',\n",
    "        ])\n",
    "    print(\"NumRows After: \"+str(df.shape[0]))\n",
    "    print(\"NumColunms After: \"+str(df.shape[1]))\n",
    "    with open(filePath+structure+\".csv\",'w') as f:\n",
    "        f.write(strucroot+\",\"+str(int(strucnum))+\"\\n\")\n",
    "        df.to_csv(f,index=False)\n",
    "    #print(subset)\n",
    "\n",
    "def dropcols(structure=\"bsc01\",filePath=\"./prepped/hcd/\",dropcols=[]):\n",
    "    print(structure)\n",
    "    strucroot=structure[:-2]\n",
    "    strucnum=structure[-2:]\n",
    "    df=pd.read_csv(filePath+structure+\".csv\",header=1)\n",
    "    print(\"NumColumns Before: \"+str(df.shape[1]))\n",
    "    df=df.drop(columns=dropcols)\n",
    "    print(\"NumColumns After: \"+str(df.shape[1]))\n",
    "    with open(filePath+structure+\".csv\",'w') as f:\n",
    "        f.write(strucroot+\",\"+str(int(strucnum))+\"\\n\")\n",
    "        df.to_csv(f,index=False)\n",
    "\n",
    "        \n",
    "#these guys already set to 99s in map, so null finder wont work above\n",
    "def bisbasparent999(structure=\"bisbas01\",filePath=\"./prepped/hcd/\"):\n",
    "    print(structure)\n",
    "    strucroot=structure[:-2]\n",
    "    strucnum=structure[-2:]\n",
    "    df=pd.read_csv(filePath+structure+\".csv\",header=1)\n",
    "    print(\"NumRows Before: \"+str(df.shape[0]))\n",
    "    df=df.loc[~(df.bissc_total==999)].copy()\n",
    "    print(\"NumRows After: \"+str(df.shape[0]))\n",
    "    with open(filePath+structure+\".csv\",'w') as f:\n",
    "        f.write(strucroot+\",\"+str(int(strucnum))+\"\\n\")\n",
    "        df.to_csv(f,index=False)\n",
    "\n",
    "#these guys already set to 99s in map, so null finder wont work above\n",
    "def neo999(structure=\"neo_ffi_form_s_adult_200301\",filePath=\"./prepped/hcd/\"):\n",
    "    print(structure)\n",
    "    strucroot=structure[:-2]\n",
    "    strucnum=structure[-2:]\n",
    "    df=pd.read_csv(filePath+structure+\".csv\",header=1)\n",
    "    print(\"NumRows Before: \"+str(df.shape[0]))\n",
    "    df=df.loc[~((df.neo_n==999)&(df.neo_e==999)&(df.neo_a==999))].copy()\n",
    "    print(\"NumRows After: \"+str(df.shape[0]))\n",
    "    with open(filePath+structure+\".csv\",'w') as f:\n",
    "        f.write(strucroot+\",\"+str(int(strucnum))+\"\\n\")\n",
    "        df.to_csv(f,index=False)\n",
    "\n",
    "def cbcl999(structure=\"cbcl01\",filePath=\"./prepped/hcd/\"):\n",
    "    print(structure)\n",
    "    strucroot=structure[:-2]\n",
    "    strucnum=structure[-2:]\n",
    "    df=pd.read_csv(filePath+structure+\".csv\",header=1)\n",
    "    print(\"NumRows Before: \"+str(df.shape[0]))\n",
    "    df=df.loc[~((df.cbcl1==999)&(df.cbcl2==999)&(df.cbcl3==999)&(df.cbcl4==999))].copy()\n",
    "    print(\"NumRows After: \"+str(df.shape[0]))\n",
    "    with open(filePath+structure+\".csv\",'w') as f:\n",
    "        f.write(strucroot+\",\"+str(int(strucnum))+\"\\n\")\n",
    "        df.to_csv(f,index=False)\n",
    "\n",
    "def cbcl1_5_999(structure=\"cbcl1_501\",filePath=\"./prepped/hcd/\"):\n",
    "    print(structure)\n",
    "    strucroot=structure[:-2]\n",
    "    strucnum=structure[-2:]\n",
    "    df=pd.read_csv(filePath+structure+\".csv\",header=1)\n",
    "    print(\"NumRows Before: \"+str(df.shape[0]))\n",
    "    df=df.loc[~((df.cbcl1==999)&(df.cbcl56a==999)&(df.cbcl_nt==999)&(df.cbcl_eye==999))].copy()\n",
    "    print(\"NumRows After: \"+str(df.shape[0]))\n",
    "    with open(filePath+structure+\".csv\",'w') as f:\n",
    "        f.write(strucroot+\",\"+str(int(strucnum))+\"\\n\")\n",
    "        df.to_csv(f,index=False)\n",
    "\n",
    "\n",
    "        \n",
    "#these guys already set to 99s in map, so null finder wont work above\n",
    "def phenx25(structure=\"phenx_su01\",filePath=\"./prepped/hcd/\"):\n",
    "    print(structure)\n",
    "    strucroot=structure[:-2]\n",
    "    strucnum=structure[-2:]\n",
    "    df=pd.read_csv(filePath+structure+\".csv\",header=1)\n",
    "    print(\"NumRows Before: \"+str(df.shape[0]))\n",
    "    df=df.loc[~((df.ale_total_number_nm==25))].copy()\n",
    "    print(\"NumRows After: \"+str(df.shape[0]))\n",
    "    with open(filePath+structure+\".csv\",'w') as f:\n",
    "        f.write(strucroot+\",\"+str(int(strucnum))+\"\\n\")\n",
    "        df.to_csv(f,index=False)\n",
    "\n",
    "def cleanlist(structurelist=['lbadl01','mchq01']):\n",
    "    for i in structurelist:\n",
    "        redcleanup(structure=i,filePath=\"./prepped/hcd/\")\n",
    "        \n",
    "def cleanzeros(structure='vitals01',filePath=\"./prepped/hcd/\"):\n",
    "    #print(structure)\n",
    "    strucroot=structure[:-2]\n",
    "    strucnum=structure[-2:]\n",
    "    df=pd.read_csv(filePath+structure+\".csv\",header=1)\n",
    "    df.loc[df.vtl007==0,'vtl007']=np.NaN\n",
    "    #df.loc[df.bp_stand=='11/80','bp_stand']=np.NaN\n",
    "    #df.loc[df.bp_stand=='9999','bp_stand']=np.NaN\n",
    "    with open(filePath+structure+\".csv\",'w') as f:\n",
    "        f.write(strucroot+\",\"+str(int(strucnum))+\"\\n\")\n",
    "        df.to_csv(f,index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.run('vitals01')\n",
    "cleanzeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test one to see if its working\n",
    "M.run('socdem01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in race and ethnicity from code that also derived ndar_subjects, as well as HCD id of parents \n",
    "#for the comqother variable\n",
    "#special clean up of annual_fam_inc vars for this structure\n",
    "singlepath=\"~/UbWinSharedSpace1/ccf-nda-behavioral/PycharmToolbox/prepped_singleton_structures/\"\n",
    "racethnic='HCPD_racethnic_for_socdem01_07_01_2020.csv'\n",
    "re=pd.read_csv(singlepath+racethnic,header=0)\n",
    "\n",
    "filePath=\"./prepped/hcd/\"\n",
    "structure='socdem01'\n",
    "strucroot='socdem'\n",
    "strucnum='01'\n",
    "df=pd.read_csv(filePath+structure+\".csv\",header=1)\n",
    "\n",
    "redf=pd.merge(re,df,on='src_subject_id',how='inner')\n",
    "redf.shape\n",
    "redf.columns\n",
    "    \n",
    "redf.loc[redf.annual_fam_inc==9999999,'annual_fam_inc']=-999999  \n",
    "redf.loc[redf.annual_fam_inc==99999999,'annual_fam_inc']=-999999\n",
    "redf.loc[redf.annual_fam_inc==9999999999,'annual_fam_inc']=-999999\n",
    "redf.loc[redf.annual_fam_inc==9999999999999,'annual_fam_inc']=-999999    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redf['newcomq']=''\n",
    "redf.loc[redf.comqother.str.contains('caregiver'),'newcomq']='caregiver '+redf.parent_id+redf.comqother.str.replace('caregiver','')\n",
    "redf.loc[redf.comqother.str.contains('adult'),'newcomq']=redf.comqother\n",
    "#redf[['comqother','parent_id','newcomq']]\n",
    "redf=redf.drop(columns=['comqother','parent_id'])\n",
    "redf=redf.rename(columns={'newcomq':'comqother'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reorder and rewrite\n",
    "redf=redf[['src_subject_id','subjectkey', 'interview_date', 'interview_age', 'sex',\n",
    "       'comqother', 'race', 'ethnicity', 'cg1_race', 'cg1_ethnicity',\n",
    "       'bio_mother', 'bio_father', 'cust_parent', 'ptner_yn','p_partnerbio',\n",
    "       'dembadpt', 'sub_adopt_1', 'country_origin', 'cg1_country_origin','ustime','cg1_ustime',\n",
    "       'nimh_rv_dem_03', 'fspgod',  'cg1_nimh_rv_dem_03', 'cg1_fspgod',\n",
    "       'das1ms',  'cg1_das1ms',\n",
    "       'area4_explain', 'employcur', 'paofwork5', 'cg1_area4_explain', 'cg1_employcur','cg1_paofwork5', \n",
    "       'ind_type', 'jobh','calm_inc1', 'sub_income','cg1_ind_type', 'dem_industry_mom_12', 'cg1_sub_income',\n",
    "        'annual_fam_inc', 'sub_income_famcode',\n",
    "       'household_number_in_house', 'preg_age_mom', 'preg_age_dad',\n",
    "       'birthcountry_dad', 'birthcountry_mom', \n",
    "        'bkgrnd_education',  'mother_edu_cat','father_edu_cat','cg1_bkgrnd_education', 'ptner_grade',\n",
    "       'family_income_dfct1', 'family_income_dfct2',\n",
    "       'family_income_dfct3', 'family_income_dfct4', 'family_income_dfct5',\n",
    "       'family_income_dfct6', 'family_income_dfct7', \n",
    "       'ptner_job', 'ptner_job1_1', 'ptner_job7_1',\n",
    "       'ptner_job8_1', 'ptner_business', 'ptner_work', 'ptner_income']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filePath+structure+\".csv\",'w') as f:\n",
    "    f.write(strucroot+\",\"+str(int(strucnum))+\"\\n\")\n",
    "    redf.to_csv(f,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont rerun socdem, since special fix applied above\n",
    "structs = [\n",
    "    'asr01',\n",
    "    'bsc01',\n",
    "    'cbcl01',\n",
    "    'cbcl1_501',\n",
    "    'cbq01',\n",
    "    'deldisk01',\n",
    "    'er4001',\n",
    "    'leap01',\n",
    "    'mab01',\n",
    "    'mctq01',\n",
    "    'medh01',\n",
    "    'mendt01',\n",
    "    'mmse01',\n",
    "    'neo_ffi_form_s_adult_200301',\n",
    "    'phenx_su01',\n",
    "    'psqi01',\n",
    "    'saiq01',\n",
    "    'sdq01',\n",
    "    'sleepdis01',\n",
    "#    'socdem01', #specialty structure\n",
    "    'vision_tests01',\n",
    "#    'vitals01',\n",
    "    'wais_iv_part101',\n",
    "    'wisc_v01',\n",
    "    'wppsiiv01',\n",
    "    'ysr01'\n",
    "]\n",
    "\n",
    "#parent report as well as self report in some cases so can be multiple rows per person\n",
    "structs2=[\n",
    "    'drugscr01',\n",
    "    'bisbas01',  \n",
    "    'eatq01',\n",
    "    'fenvs01',\n",
    "    'gbi01',\n",
    "    'pds01',\n",
    "    'scan_debrief01',\n",
    "    'srs02',\n",
    "    'upps01',\n",
    "    'screentime01'  \n",
    "]\n",
    "for s in structs:\n",
    "    M.run(s)\n",
    "\n",
    "for s in structs2:\n",
    "    M.run(s)\n",
    "#for i in structs:\n",
    "#    print(i)\n",
    "#for i in structs2:\n",
    "#    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#now clean up empty rows\n",
    "cleanlist(structurelist=structs)\n",
    "cleanlist(structurelist=structs2)\n",
    "redcleanup(structure=\"asr01\",filePath=\"./prepped/hcd/\",extraomitcol1='somatic_complaints_total',extraomitcol2='missingsum_obvq',extraomitcol3='kksomp') #line is redundant with cleanlist function\n",
    "asr(structure=\"asr01\",filePath=\"./prepped/hcd/\")\n",
    "dropcols(structure=\"bsc01\",filePath=\"./prepped/hcd/\",dropcols=['comments'])\n",
    "bisbasparent999(structure=\"bisbas01\",filePath=\"./prepped/hcd/\")\n",
    "redcleanup(structure='cbcl01',filePath=\"./prepped/hcd/\",extraomitcol1='version_form')\n",
    "cbcl999(structure=\"cbcl01\",filePath=\"./prepped/hcd/\")\n",
    "cbcl1_5_999(structure=\"cbcl1_501\",filePath=\"./prepped/hcd/\")\n",
    "redcleanup(structure='deldisk01',filePath=\"./prepped/hcd/\",extraomitcol1='version_form')\n",
    "redcleanup(structure=\"eatq01\",filePath=\"./prepped/hcd/\",extraomitcol1='respond')\n",
    "redcleanup(structure='gbi01',filePath=\"./prepped/hcd/\",extraomitcol1='version_form',extraomitcol2='sup_y_ss_sum_nm')\n",
    "redcleanup(structure='mctq01',filePath=\"./prepped/hcd/\",extraomitcol1='version_form',extraomitcol2='frprnts')\n",
    "neo999(structure=\"neo_ffi_form_s_adult_200301\",filePath=\"./prepped/hcd/\")\n",
    "redcleanup(structure=\"pds01\",filePath=\"./prepped/hcd/\",extraomitcol1='respond')\n",
    "phenx25(structure=\"phenx_su01\",filePath=\"./prepped/hcd/\")\n",
    "redcleanup(structure='srs02',filePath=\"./prepped/hcd/\",extraomitcol1='respond',extraomitcol2='respond_detail',extraomitcol3='phenotype')\n",
    "redcleanup(structure='upps01',filePath=\"./prepped/hcd/\",extraomitcol1='version_form')\n",
    "redcleanup(structure='ysr01',filePath=\"./prepped/hcd/\",extraomitcol1='version_form',extraomitcol2='missingsum_obvq')\n",
    "ysrcols=['cbcl1_2_text','cbcl2_3_text','cbcl5_2_text','cbcl6_6_text','cbcl7_3_text','cbcl9_2_text','cbcl10_1_text',\n",
    "    'cbcl11_2_text','cbcl11_6_text','cbcl12_6_text','cbcl13_1_text','cbcl13_5_text','cbcl13_6_text','cbcl13_7_text',\n",
    "    'cbcl16_1_text','cbcl16_6_text','cbcl56h_des']\n",
    "dropcols(structure=\"ysr01\",filePath=\"./prepped/hcd/\",dropcols=ysrcols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NDA_submissions_venv",
   "language": "python",
   "name": "nda_submissions_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
