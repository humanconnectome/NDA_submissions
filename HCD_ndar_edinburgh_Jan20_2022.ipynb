{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HCD_ndar_edinburgh_2022release (Lifespan 3.0)\n",
    "#add extra export statement to put race and ethnicity of parents in separate file for socdem01 pickup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime\n",
    "import csv\n",
    "import sys\n",
    "import shutil\n",
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from scipy import stats\n",
    "from ccf.box import LifespanBox\n",
    "from ccf.config import LoadSettings\n",
    "from ccf.redcap import RedcapTable \n",
    "config = LoadSettings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box=LifespanBox(cache='./')\n",
    "verbose = True\n",
    "snapshotdate = datetime.datetime.today().strftime('%m_%d_%Y')\n",
    "pathout=\"./prepped/hcd\" \n",
    "racepath=\"./prepped\"\n",
    "\n",
    "#Rosetta (a.k.a Inventory) file will have all the nda vars and pedids\n",
    "extrainfo=config['rosetta']['filename']\n",
    "\n",
    "eventlist=['visit_1_arm_1','visit_2_arm_1','visit_3_arm_1','visit_arm_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory=pd.read_csv(extrainfo)\n",
    "inventory.columns\n",
    "inventory=inventory.loc[inventory.nda_age.isnull()==False]\n",
    "\n",
    "Fullinventory=inventory.copy()\n",
    "print(\"Full Inventory w All events:\",Fullinventory.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory.loc[inventory.nda_age>300][['nda_age','subject','event_age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#extract the inventory subset \n",
    "inventory=pd.read_csv(extrainfo)[['DB_Source','REDCap_id_parent','subject','parent_at_visit', 'parent_at_V1','ParentPIN','pseudo_guid','nda_interview_date','nda_age','M/F','pedid','redcap_event_name','redcap_event','REDCap_id','race','ethnic_group','site']]\n",
    "inventory=inventory.loc[inventory.nda_age.isnull()==False]\n",
    "inventory.nda_interview_date=pd.to_datetime(inventory.nda_interview_date).dt.strftime('%m/%d/%Y')\n",
    "inventory.nda_age=inventory.nda_age.round(0).astype(int)\n",
    "print('Inventory Shape out of the Box',inventory.shape)\n",
    "\n",
    "#These subjects can't go to NDA.  \n",
    "#We don't have any of the necessary NDA variables for these ParentPIN subjects who showed up for NIH Toolbox sessions but not REDCap\n",
    "inventory.loc[~(inventory.parent_at_visit==inventory.ParentPIN.str[0:10]) & (inventory.ParentPIN.isnull()==False)]\n",
    "\n",
    "p=inventory.loc[inventory.parent_at_V1.isnull()==False]\n",
    "print(\"Number of Unique Subjects\",len(inventory.subject.unique()))\n",
    "print(\"Number of Unique Parents\",len(p.parent_at_V1.unique()))  \n",
    "\n",
    "##these guys, too, have minimal information.  \n",
    "##will place all toolbox parent about self data under parent_at_V1 or omit completely from NDA push\n",
    "##inventory.loc[~(inventory.parent_at_visit==inventory.parent_at_V1) & (inventory.redcap_event_name.isin(['visit_1_arm_1','visit_2_arm_1','visit_3_arm_1']))]\n",
    "sublist=list(inventory.subject.unique())+list(p.parent_at_V1.unique())\n",
    "d=pd.DataFrame(sublist,columns=['subject'])\n",
    "d.subject.head()\n",
    "\n",
    "d=d.loc[~(d.subject.isnull()==True)]\n",
    "subjectlist=list(d.subject)\n",
    "len(subjectlist)\n",
    "print('Number of parent_at_V1+subjects not NAN',len(subjectlist))\n",
    "\n",
    "print('inventory shape',inventory.shape)\n",
    "inventory=inventory.loc[~(inventory.subject.isnull()==True)]\n",
    "print('inventory shape',inventory.shape)\n",
    "\n",
    "print('inventory shape',inventory.shape)\n",
    "p=inventory.loc[~(inventory.parent_at_V1.isnull()==True)]\n",
    "print('inventory shape',p.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare ped file for merging with ndar subjects (first parents, then children and teens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#curated pedigrees contain redundant information with inventory (e.g. pedids and pseudoguids, HCA_ID, HCD_ID, etc).\n",
    "#however need to keep sex of parents.  \n",
    "peds=883930709978\n",
    "pathpeds=box.downloadFile(peds)\n",
    "pedids=pd.read_csv(pathpeds)\n",
    "pedids=pedids.drop(columns=['Familyflag','HCD_ID','HCA_ID','site','parent','age','relationship1', 'with1', 'relationship2', 'with2', 'relationship3',\n",
    "       'with3', 'relationship4', 'with4', 'relationship5', 'with5', 'src_avuncular1_id','src_grandparent1_id','src_cousin_id1','src_cousin_id2',\n",
    "       'relationship6', 'with6', 'relationship7', 'with7'])\n",
    "pedids.loc[pedids.subject.str.contains('HCD')].head()\n",
    "pedids.loc[pedids.subject.str.contains('HCD')].shape\n",
    "\n",
    "#these are all second parents\n",
    "pedids.loc[~(pedids.subject.isin(subjectlist)) & (pedids.subject.str.contains('HCD'))]\n",
    "pedids.shape\n",
    "#pedids.loc[(pedids.subject.str.contains('HCD')) & (pedids.sibling_type1_alt.isnull()==False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these special cases (unspecified half sibs, and twins) need to be noted in release notes somewhere - already noted in family flag internally\n",
    "#e.g. subj is list of people who have non-conforming relationship type w.r.t siblings that NDA can't handle\n",
    "subj=list(pedids.loc[(pedids.subject.str.contains('HCD')) & (pedids.sibling_type1_alt.isin(['HS','FTS','FTB'])),'subject']) #HCD0295146,HCD2685169,HCD2971776,HCD1197757\n",
    "#['HCD0295146->FB', 'HCD2685169->FS', 'HCD2971776->HS', 'HCD1197757->HS']\n",
    "pedids['zygosity']=''\n",
    "pedids.loc[(pedids.subject.str.contains('HCD')) & (pedids.sibling_type1_alt.isin(['HS'])),'sibling_type1']='HMS'\n",
    "pedids.loc[(pedids.subject.str.contains('HCD')) & (pedids.sibling_type1_alt.isin(['FTB'])),'sibling_type1']='FB'\n",
    "pedids.loc[(pedids.subject.str.contains('HCD')) & (pedids.sibling_type1_alt.isin(['FTB'])),'zygosity']='dizygous'\n",
    "\n",
    "pedids.loc[(pedids.subject.str.contains('HCD')) & (pedids.sibling_type1_alt.isin(['FTS'])),'sibling_type1']='FS'\n",
    "pedids.loc[(pedids.subject.str.contains('HCD')) & (pedids.sibling_type1_alt.isin(['FTS'])),'zygosity']='dizygous'\n",
    "\n",
    "#pedids.loc[pedids.subject.isin(subj)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#now deal with adoptive parents to get themm into NDA format\n",
    "pedids2=pedids.rename(columns={'biomomsubj':'src_mother_id','biodadsubj':'src_father_id'}).copy()\n",
    "adopts=pedids2.loc[(pedids2.adoptmomsubj.isnull()==False)|(pedids2.adoptdadsubj.isnull()==False)].copy()# & (pedids.biomomsubj=='0')]\n",
    "adopts.head(2)\n",
    "adoptsubj=list(adopts.subject)\n",
    "#adoptsubj\n",
    "#['HCD0236130',\n",
    "# 'HCD0486254',\n",
    "# 'HCD0406230',\n",
    "# 'HCD0977576',\n",
    "# 'HCD0822850',\n",
    "# 'HCD1021821',\n",
    "# 'HCD0075435',\n",
    "# 'HCD2491257',\n",
    "# 'HCD2502640',\n",
    "# 'HCD0039128',\n",
    "# 'HCD2044333',\n",
    "# 'HCD0146331',\n",
    "# 'HCD1170131',\n",
    "# 'HCD1331432',\n",
    "# 'HCD0868268']\n",
    "pedids2.loc[(pedids2.adoptmomsubj.isnull()==False) & (pedids2.src_mother_id=='0'),'src_mother_id']=pedids2.adoptmomsubj\n",
    "pedids2.loc[(pedids2.adoptdadsubj.isnull()==False) & (pedids2.src_father_id=='0'),'src_father_id']=pedids2.adoptdadsubj\n",
    "pedids2.loc[(pedids2.adoptmomsubj.isnull()==False) & (pedids2.src_mother_id.str.contains('DMOM')),'src_mother_id']=pedids2.adoptmomsubj\n",
    "pedids2.loc[(pedids2.adoptdadsubj.isnull()==False) & (pedids2.src_father_id.str.contains('DDAD')),'src_father_id']=pedids2.adoptdadsubj\n",
    "\n",
    "pedids2.loc[(pedids2.subject.isin(adoptsubj)) & (pedids2.src_father_id=='0'),'src_father_id']='DDAD1_'+ pedids2.pedid.astype(str) #,'src_father_id']=pedids.\n",
    "pedids2.loc[(pedids2.subject.isin(adoptsubj)) & (pedids2.src_mother_id=='0'),'src_mother_id']='DMOM1_'+ pedids2.pedid.astype(str) #,'src_father_id']=pedids.\n",
    "\n",
    "pedids2.loc[(pedids2.subject=='HCD0486254') ,'src_father_id']='DDAD2_5073'\n",
    "pedids2.head()\n",
    "\n",
    "pedids2.loc[pedids2.subject.str.contains('HCD')].shape\n",
    "\n",
    "#s=pd.DataFrame(subjectlist,columns=['subject'])\n",
    "#s.head()\n",
    "#peds3=pd.merge(s,pedids2,on='subject',how='outer',indicator=True)\n",
    "#peds3._merge.value_counts()#head()\n",
    "#peds3.loc[peds3._merge=='left_only']\n",
    "#peds3.subject.value_counts()\n",
    "pedids3=pedids2.loc[pedids2.subject.isin(subjectlist)]\n",
    "pedids3.loc[pedids3.subject.str.contains('HCD')].shape\n",
    "#pedids3.loc[pedids3.subject==''].shape\n",
    "#pedids3.shape\n",
    "#pedids2.shape\n",
    "#len(subjectlist)\n",
    "#for i in subjectlist:\n",
    "#    print(i)  \n",
    "    #if 'HCD' not in i:\n",
    "    #    print(i)\n",
    "    #else:\n",
    "    #    print(i,\"wierd\")\n",
    "        \n",
    "#subjectlist.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Deal with Parents; Get them in to ndar_subjets format for stacking with children and teens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "parentinventory=inventory.drop(columns=['parent_at_visit','race','ethnic_group','pedid','M/F','nda_age','pseudo_guid','ParentPIN']).rename(columns={'subject':'comqother'})\n",
    "parentinventory.comqother=\"Parent about self as context for \" + parentinventory.comqother\n",
    "parentinventory=parentinventory.rename(columns={'parent_at_V1':'subject'})\n",
    "print(parentinventory.shape)\n",
    "\n",
    "#p=      inventory.loc[~(inventory.parent_at_V1.isnull()==True)]\n",
    "p=parentinventory.loc[~(parentinventory.subject.isnull()==True)]\n",
    "print('parentinventory #unique parents',len(p.subject.unique()))\n",
    "\n",
    "parentinventory=parentinventory.loc[parentinventory.DB_Source.isin(['parent_only','parentandchild'])]\n",
    "alt=parentinventory.loc[parentinventory.subject.isin(list(p.subject.unique()))]\n",
    "\n",
    "print('DB_Source counts',parentinventory.shape)\n",
    "print('Alt counts',alt.shape)\n",
    "\n",
    "parentinventory.head()\n",
    "parentinventory.loc[parentinventory.subject.isnull()==True]\n",
    "a=parentinventory.loc[~(parentinventory.subject.isnull()==True)]\n",
    "print('parentinventory shape',len(a.subject.unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pedids3.head()\n",
    "#parentinventory.head()\n",
    "pedids3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge ped vars with inventory (inventory already has pedid) by parent_at_V1 to get the parents some data\n",
    "pedids3=pedids3.drop(columns=['adoptmomsubj','adoptdadsubj','sibling_type1_alt'])\n",
    "\n",
    "pedids3.head()\n",
    "print('parentinventory shape before',parentinventory.shape)\n",
    "parentinventory2=pd.merge(parentinventory,pedids3,left_on=['subject'],right_on=['subject'],how='inner')\n",
    "print('parentinventory shape after',parentinventory2.shape)\n",
    "parentinventory2.loc[parentinventory2.psuedo_guid=='NDAR_INVGR630MPG']\n",
    "print(\"number of unique parents\",len(parentinventory2.subject.unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get parent race ethnicity and ages.  Use pseudo_guid and sex from pedid because otherwise will only grab from V1\n",
    "#ndar subjects needs required vars for regular and NIH Toolbox guest appearance parents\n",
    "hcdparent=RedcapTable.get_table_by_name('hcpdparent').get_frame(['parent_age','redcap_event_name','id'])\n",
    "print(hcdparent.describe())\n",
    "dfnewparent=hcdparent.copy()\n",
    "dfnewparent.head()\n",
    "#hcdparent=RedcapTable.get_table_by_name('hcpdparent').get_frame(['parent_age','redcap_event_name','id','p_race','p_latino'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parentinventory2.columns\n",
    "dfnewparent.columns\n",
    "print(dfnewparent.shape)\n",
    "dfnewparent.drop_duplicates(subset=['id','redcap_event_name']).shape\n",
    "\n",
    "print(\"number of unique parents\",len(parentinventory2.subject.unique()))\n",
    "parentinventory2.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign missing values for ages\n",
    "#store mean for missing values after inference\n",
    "meanparentageV1=pd.DataFrame(dfnewparent.loc[dfnewparent.redcap_event_name=='visit_1_arm_1','parent_age'].describe()).loc['mean'][0].round(0)\n",
    "\n",
    "#merging by id assures that we're just going with parent_at_V1 for purpose of GUIDS and ages.  Other parents dont have REDCap records. \n",
    "#not that the shape reduction of 78 observations corresponds with covid_2_arm_1 and followup_arm_1 with empty values for id\n",
    "\n",
    "inventory2=pd.merge(parentinventory2,dfnewparent,left_on=['REDCap_id_parent','redcap_event_name'],right_on=['id','redcap_event_name'],how='inner')\n",
    "inventory2[['subject']]\n",
    "print('Inventory2 shape after merging with REDCap',inventory2.shape)\n",
    "\n",
    "#find those with missing ages\n",
    "p1=inventory2.loc[(inventory2.parent_age.isnull()==True)]# & (inventory2.redcap_event_name.isin(['visit_1_arm_1','visit_2_arm_1','visit_3_arm_1']))]\n",
    "\n",
    "#get those with missing ages and their NDA variables\n",
    "p3=inventory2.loc[inventory2.id.isin(list(p1.REDCap_id_parent))][['redcap_event_name','id','parent_age','nda_interview_date']]\n",
    "#p3['nda_interview_date']=pd.to_datetime(p3['nda_interview_date'])\n",
    "p4=p3.sort_values(by=['id','nda_interview_date']).copy()\n",
    "p4.head()\n",
    "#inventory2.head()\n",
    "#inventory2.shape\n",
    "p4.shape\n",
    "inventory2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory2['nda_interview_date']=pd.to_datetime(inventory2['nda_interview_date'],errors='coerce')\n",
    "inventory2.nda_interview_date.dt.strftime('%Y').astype(int)\n",
    "inventory2.nda_interview_date.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in missing ages\n",
    "\n",
    "p6=pd.DataFrame()\n",
    "for sub in list(inventory2.REDCap_id_parent.unique()):\n",
    "    #print(sub)\n",
    "    p4=inventory2.copy()\n",
    "    p5=p4.loc[p4.REDCap_id_parent==sub].copy()\n",
    "    p5['oldage']=p5.parent_age\n",
    "    #initialize\n",
    "    p5['newage']=0.0\n",
    "    p5['years']=p5.nda_interview_date.dt.strftime('%Y').astype(int)\n",
    "    p5['lastage']=p5.parent_age.shift(+1)\n",
    "    p5['lastyear']=p5.years.shift(+1)\n",
    "    p5['nextage']=p5.parent_age.shift(-1)\n",
    "    p5['nextyear']=p5.years.shift(-1)\n",
    "    #iterate forward and backwards a few times to fill in missing ages when it can be inferred from other visits\n",
    "    for x in range(0, 4):\n",
    "        #forward\n",
    "        p5.loc[(p5.parent_age.isnull()==True),'newage']=p5.lastage + (p5.years-p5.lastyear)\n",
    "        p5.loc[p5.parent_age.isnull()==True,'parent_age']=p5.newage\n",
    "        p5['lastage']=p5.parent_age.shift(+1)\n",
    "        p5['lastyear']=p5.years.shift(+1)\n",
    "        #p5.loc[p5.parent_age.isnull()==True,'newage']=p5.lastage + (p5.years-p5.lastyear)\n",
    "        #p5.loc[p5.parent_age.isnull()==True,'parent_age']=p5.newage       \n",
    "        #backwards\n",
    "        p5.loc[p5.parent_age.isnull()==True,'newage']=p5.nextage - (p5.nextyear-p5.years)\n",
    "        p5.loc[p5.parent_age.isnull()==True,'parent_age']=p5.newage\n",
    "        p5['nextage']=p5.parent_age.shift(-1)\n",
    "        p5['nextyear']=p5.years.shift(-1)\n",
    "    #initialize with the mean for any subjects still missing ages\n",
    "    for x in range(0,1):\n",
    "        #print(sub)\n",
    "        p5.loc[(p5.parent_age.isnull()==True) & (p5.redcap_event_name=='visit_1_arm_1'),'newage']=meanparentageV1\n",
    "        p5.loc[(p5.parent_age.isnull()==True) & (p5.redcap_event_name=='visit_1_arm_1')==True,'parent_age']=p5.newage\n",
    "        p5['lastage']=p5.parent_age.shift(+1)\n",
    "        p5['lastyear']=p5.years.shift(+1)\n",
    "        p5.loc[(p5.parent_age.isnull()==True),'newage']=p5.lastage + (p5.years-p5.lastyear)\n",
    "        p5.loc[p5.parent_age.isnull()==True,'parent_age']=p5.newage\n",
    "    p6=pd.concat([p6,p5],axis=0)\n",
    " \n",
    "p6['parent_nda_age']=12 * p6.parent_age\n",
    "print(p6.shape)\n",
    "inventory2.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p6.loc[p6.parent_age.isnull()==True]\n",
    "p6.shape\n",
    "#p6.head()\n",
    "#inventory2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p7=p6[['redcap_event_name','id','parent_age']].rename(columns={'id':'REDCap_id_parent'})\n",
    "inventory3=pd.merge(inventory2,p7,how='left',on=['REDCap_id_parent','redcap_event_name'])\n",
    "print('together ages:',inventory3.shape)\n",
    "#\n",
    "inventory3.loc[(inventory3.parent_age_x.isnull()==True) & (inventory3.redcap_event_name),'parent_age_x']=inventory3.parent_age_y\n",
    "inventory3=inventory3.rename(columns={'parent_age_x':'parent_age'}).drop(columns=['parent_age_y'])\n",
    "inventory3.columns\n",
    "print('merge again:',inventory3.shape)\n",
    "\n",
    "inventory3=inventory3.drop(columns=['id'])\n",
    "#rename back to orig for next stesp\n",
    "inventory=inventory3.copy()\n",
    "inventory.columns\n",
    "#inventory.shape\n",
    "#inventory.head()\n",
    "inventory.loc[inventory.parent_age.isnull()==True]\n",
    "print('one more check:',inventory3.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the method used to derive children's categorical races from checkbox varialbe in the inventory ([config][rosetta]). Implies that someone self-reporting as Asian and White is Multiple races.  This is a limitation, as there are many individuals who would check 'Asian' on a categorical variable but 'Asian and White' on a checkbox variable.  Distributional consequences.\n",
    "\n",
    "Child races have already been calculated and stored in inventory.  Parental races not calculated for the inventory, so added here.\n",
    "\n",
    "12, NATIVE AMERICAN | 18, ASIAN |  11, BLACK/AFRICAN AMERICAN |  14, NATIVE HAWAIIAN or\n",
    "#OTHER PACIFIC ISLANDER |10, WHITE | 25, MORE THAN ONE RACE |  99, DON'T KNOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get races\n",
    "racevars=RedcapTable.get_table_by_name('hcpdparent').get_frame(['redcap_event_name','parent_id','child_id','id','p_race','p_latino'])\n",
    "racevars=racevars.loc[racevars.redcap_event_name=='visit_1_arm_1'].copy()\n",
    "racevars.head()\n",
    "#print(racevars.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "racevars['p_sumcheckedrace']=racevars[['p_race___10','p_race___11','p_race___12','p_race___14','p_race___18','p_race___25']].sum(axis=1)\n",
    "racevars.loc[(racevars.p_sumcheckedrace==1) & (racevars.p_race___10==1),'p_racial']=5\n",
    "racevars.loc[(racevars.p_sumcheckedrace==1) & (racevars.p_race___11==1),'p_racial']=3\n",
    "racevars.loc[(racevars.p_sumcheckedrace==1) & (racevars.p_race___12==1),'p_racial']=1\n",
    "racevars.loc[(racevars.p_sumcheckedrace==1) & (racevars.p_race___14==1),'p_racial']=4\n",
    "racevars.loc[(racevars.p_sumcheckedrace==1) & (racevars.p_race___18==1),'p_racial']=2\n",
    "racevars.loc[(racevars.p_sumcheckedrace==1) & (racevars.p_race___25==1),'p_racial']=6\n",
    "racevars.loc[(racevars.p_sumcheckedrace==0) & (racevars.p_race___99==1),'p_racial']=99\n",
    "racevars.loc[racevars.p_sumcheckedrace>=2,'p_racial']=6\n",
    "\n",
    "racevars.loc[racevars.p_racial.isnull()==True,'p_racial']=99\n",
    "racevars.loc[racevars.p_latino.isnull()==True,'p_latino']=9\n",
    "\n",
    "racevars.p_racial\n",
    "racevars.p_racial.value_counts(dropna=False)\n",
    "#racevars.p_sumcheckedrace.value_counts(dropna=False)\n",
    "racevars.p_latino.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "racevars['p_race']=racevars.replace({'p_racial':\n",
    "                                       {1.0:'American Indian/Alaska Native',\n",
    "                                        2.0:'Asian',\n",
    "                                        3.0:'Black or African American',\n",
    "                                        4.0:'Hawaiian or Pacific Islander',\n",
    "                                        5.0:'White',\n",
    "                                        6.0:'More than one race',\n",
    "                                        99.0:'Unknown or not reported'}})['p_racial']\n",
    "racevars['p_ethnic_group']=racevars.replace({'p_latino':\n",
    "                                           {1.0:'Hispanic or Latino',\n",
    "                                            0.0:'Not Hispanic or Latino',\n",
    "                                            9.0:'Unknown or not reported'}})['p_latino']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "racevars.p_race.value_counts(dropna=False)\n",
    "#racevars.p_ethnic_group.value_counts(dropna=False)\n",
    "racevars.columns\n",
    "print(Fullinventory.columns) #inventory only has parents, and you need child subject for merge\n",
    "print(racevars.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write out csv with parental race and ethnicity information for pulling into socdem\n",
    "socdem=racevars[['child_id', 'p_race','p_ethnic_group', 'parent_id']].copy() # 'subjectkey','interview_date', 'interview_age','site',\n",
    "extras=Fullinventory.loc[Fullinventory.redcap_event=='V1'][['DB_Source','subject','nda_age','race','ethnic_group','parent_at_V1','redcap_event_name']]\n",
    "socdem=pd.merge(socdem,extras,left_on='child_id',right_on='subject',how='right')#,indicator=True)\n",
    "socdem.DB_Source.value_counts()\n",
    "socdem=socdem.rename(columns={'subject':'src_subject_id','p_race':'cg1_race','p_ethnic_group':'cg1_ethnicity','ethnic_group':'ethnicity'})\n",
    "socdem.loc[(socdem.cg1_race.isnull()==True) & (socdem.nda_age.astype(int) < 216)][['cg1_race']]#='Unknown or not reported'\n",
    "socdem.to_csv(racepath+'/HCPD_racethnic_for_socdem01_' +snapshotdate+'.csv',index=False)\n",
    "#socdem.shape\n",
    "\n",
    "#Fullinventory.loc[Fullinventory.redcap_event=='V1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "racevars=racevars.drop(columns=['p_latino',\n",
    "       'p_race___10', 'p_race___11', 'p_race___12', 'p_race___14',\n",
    "       'p_race___18', 'p_race___25', 'p_race___99', \n",
    "        'p_sumcheckedrace','p_racial','redcap_event_name'])\n",
    "\n",
    "racevars=racevars.rename(columns={'p_race':'race','p_ethnic_group':'ethnic_group'})\n",
    "racevars.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parentinventoryfinal=pd.merge(inventory,racevars,left_on='REDCap_id_parent',right_on='id',how='left')\n",
    "\n",
    "parentinventoryfinal['M/F']=parentinventoryfinal.replace({'sex':\n",
    "                                           {1.0:'M',\n",
    "                                            2.0:'F',\n",
    "                                            }})['sex']\n",
    "\n",
    "parentinventoryfinal.sex.value_counts(dropna=False)\n",
    "parentinventoryfinal['M/F'].value_counts(dropna=False)\n",
    "#nda age in months\n",
    "parentinventoryfinal['nda_age'] = 12 * parentinventoryfinal.parent_age\n",
    "\n",
    "parentinventoryfinal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parentinventoryfinal (above) is the frame to stack with children for ndar subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now create the child/teen data frame, which will have ndar variables as well as edinburgh variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pedids3.columns\n",
    "extrainfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "childinventory=Fullinventory\n",
    "\n",
    "print(childinventory.shape)\n",
    "childinventory=childinventory.loc[childinventory.DB_Source.isin(['child_only','teen','parentandchild'])]\n",
    "print(childinventory.shape)\n",
    "childinventory=childinventory.loc[childinventory.nda_age.isnull()==False]\n",
    "print(childinventory.shape)\n",
    "#inventory.nda_interview_date=pd.to_datetime(inventory.nda_interview_date).dt.strftime('%m/%d/%Y')\n",
    "#inventory.nda_age=inventory.nda_age.round(0).astype(int)\n",
    "childinventory.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pedids3.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat children/teens and parents together for NDAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merge with pedids3\n",
    "childinventory2=pd.merge(childinventory.drop(columns='pedid'),pedids3,how='left',on='subject')#,indicator=True)\n",
    "#print(childinventory2._merge.value_counts())\n",
    "#print(childinventory2.columns)\n",
    "#print(parentinventoryfinal.columns)\n",
    "\n",
    "keeplist=['DB_Source','subject','nda_interview_date','nda_age','M/F',\n",
    "          'redcap_event_name','redcap_event','race','ethnic_group',\n",
    "          'site', 'psuedo_guid', 'pedid','src_mother_id',\n",
    "          'src_father_id','src_sibling1_id',\n",
    "          'sibling_type1', 'src_sibling2_id',\n",
    "          'sibling_type2', 'src_sibling3_id', 'sibling_type3', 'zygosity']\n",
    "\n",
    "childinventory3=childinventory2[keeplist+['REDCap_id']].copy()\n",
    "parentinventory3=parentinventoryfinal[keeplist].copy()\n",
    "#print('childinventoryshape',childinventory3.shape)\n",
    "#print('parentinventoryshape',parentinventory3.shape)\n",
    "\n",
    "c=childinventory3.reset_index().copy()\n",
    "p=parentinventory3.reset_index().copy()\n",
    "ndarinventory=pd.concat([c,p],axis=0)\n",
    "#print(ndarinventory.shape)\n",
    "ndarinventory=ndarinventory.rename(columns={\"subject\":\"src_subject_id\",\"M/F\":\"gender\",\"nda_age\":\"interview_age\",\n",
    "                           \"nda_interview_date\":\"interview_date\",\n",
    "                           \"pedid\":\"family_user_def_id\",\"psuedo_guid\":\"subjectkey\",'redcap_event':'visit'})\n",
    "\n",
    "ndarinventory['phenotype']=pd.Series((\"Healthy Subject\" for i in range(ndarinventory.shape[0])),index=ndarinventory.index)\n",
    "ndarinventory['phenotype_description']=pd.Series((\"In good health\" for i in range(ndarinventory.shape[0])),index=ndarinventory.index)\n",
    "ndarinventory['twins_study']=pd.Series((\"No\" for i in range(ndarinventory.shape[0])),index=ndarinventory.index)\n",
    "ndarinventory['sibling_study']=pd.Series((\"No\" for i in range(ndarinventory.shape[0])),index=ndarinventory.index)\n",
    "ndarinventory['family_study']=pd.Series((\"No\" for i in range(ndarinventory.shape[0])),index=ndarinventory.index)\n",
    "ndarinventory['sample_taken']=pd.Series((\"No\" for i in range(ndarinventory.shape[0])),index=ndarinventory.index)\n",
    "\n",
    "\n",
    "ndarinventory=ndarinventory.loc[ndarinventory.redcap_event_name.isin(eventlist)]\n",
    "ndarinventory=ndarinventory.drop(columns=['DB_Source','REDCap_id','redcap_event_name'])\n",
    "print(ndarinventory.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndarinventory.interview_date=pd.to_datetime(ndarinventory.interview_date).dt.strftime('%m/%d/%Y')\n",
    "ndarinventory.interview_age=ndarinventory.interview_age.round(0).astype(int)\n",
    "siblist=['sibling_type1','sibling_type2','sibling_type3']\n",
    "for ii in siblist:\n",
    "    ndarinventory[ii]=ndarinventory.replace({ii:\n",
    "                                       {'FB':'Full Brother FB',\n",
    "                                        'HMB':'Half Mother Brother HMB',\n",
    "                                        'HFB':'Half Father Brother HFB',\n",
    "                                        'FS':'Full Sister FS',\n",
    "                                        'HMS':'Half Mother Sister HMS',\n",
    "                                        'HFS':'Half Father Sister HFS',\n",
    "                                        'AB':'Adopted Brother AB',\n",
    "                                        'AS':'Adopted Sister AS',\n",
    "                                       }})[ii]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndarinventory=ndarinventory.drop(columns=['index'])\n",
    "#p.shape\n",
    "#c.shape\n",
    "print(p.columns)\n",
    "print(c.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write out csv for validation\n",
    "filePath=pathout+'/ndar_subject01.csv'\n",
    "\n",
    "if os.path.exists(filePath):\n",
    "    os.remove(filePath)\n",
    "else:\n",
    "    print(\"Can not delete the file as it doesn't exists\")\n",
    "\n",
    "with open(filePath,'a') as f:\n",
    "    f.write(\"ndar_subject,1\\n\")\n",
    "    ndarinventory.to_csv(f,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#get handedness variables and ids from Redcap child\n",
    "fieldlist=['hand1','hand2','hand3','hand4','hand5','hand6','hand7','hand8',\n",
    "           'hand_total','iihandwr','iihandth','iihandsc','iihandto','iihandkn','iihandsp',\n",
    "           'iihandbr','iihandma','iihandbo','iihandfk','iihandey','redcap_event_name','id','subject_id']\n",
    "hcdchild=RedcapTable.get_table_by_name('hcpdchild').get_frame(fieldlist)\n",
    "hcdchild.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hcdchild.value_counts('redcap_event_name')\n",
    "\n",
    "childinventory3.loc[childinventory3[\"redcap_event_name\"].isin(eventlist)].value_counts('redcap_event_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge child handedness with inventory\n",
    "hcdchildI=pd.merge(childinventory3.loc[childinventory3[\"redcap_event_name\"].isin(eventlist)],hcdchild,left_on=['REDCap_id','redcap_event_name'],right_on= ['id','redcap_event_name'],how='left')\n",
    "#drop teens\n",
    "hcdchildI=hcdchildI.loc[~(hcdchildI.redcap_event_name=='visit_arm_1')]\n",
    "hcdchildI.value_counts('redcap_event_name')\n",
    "#hcdchildI.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##hcd18 and older\n",
    "list18=['iihandwr', 'iihandth', 'iihandsc', 'iihandto', 'iihandkn', 'iihandsp',\n",
    "        'iihandbr', 'iihandma', 'iihandbo', 'iihandfk', 'iihandey','redcap_event_name','id']\n",
    "\n",
    "hcpd18=RedcapTable.get_table_by_name('hcpd18').get_frame(list18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcpd18I=pd.merge(childinventory3.loc[childinventory3[\"redcap_event_name\"].isin(['visit_arm_1'])],hcpd18,left_on=['REDCap_id','redcap_event_name'],right_on= ['id','redcap_event_name'],how='left')\n",
    "hcpd18I.value_counts('redcap_event_name')\n",
    "#hcpd18I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate hcdchild and hcd18...\n",
    "hcpd18I.columns #18 and older dataframe\n",
    "hcdchildI.columns #(child dataframe)\n",
    "hcdtogether=pd.concat([hcpd18I,hcdchildI],axis=0,sort=True)\n",
    "hcdtogether.to_csv('test.csv')\n",
    "hcdtogether.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't concat parents with children\n",
    "#NO:concatenate parents with children\n",
    "#parentinventory3['Source']='parent'\n",
    "#hcdtogether['Source']='childteen'\n",
    "#HCD=pd.concat([parentinventory3,hcdtogether],axis=0)\n",
    "HCD=hcdtogether.copy()\n",
    "HCD.columns\n",
    "HCD.DB_Source.value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcdtogether2=HCD.copy()\n",
    "hcdtogether2=hcdtogether2.rename(columns={\"subject\":\"src_subject_id\",\"M/F\":\"gender\",\"nda_age\":\"interview_age\",\n",
    "                           \"nda_interview_date\":\"interview_date\",\"psuedo_guid\":\"subjectkey\",\"redcap_event\":\"visit\"})\n",
    "hcdtogether2['DB_Source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flagsz=hcdtogether2.loc[hcdtogether2.race.isnull()==True]\n",
    "#flags=flagsz[['subject_id','site_char','nda_interview_date']].copy()\n",
    "#flags.loc[:,'reason']=pd.Series((\"No race\" for i in range(flags.shape[0])),index=flags.index)\n",
    "flagsz\n",
    "#hcdtogether2.loc[hcdtogether2.ethnic_group.isnull()==True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcdtogether2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hand\n",
    "#last minute translation to valuess now requrested by NDA 4/8/2019\n",
    "#spr;right;spl;left;both;np;ne; oh\n",
    "#SPR=Strongly Prefer Right; SPL=Strongly Prefer Left; ne =not experienced; np=no preference; oh=sometimes uses other hand\n",
    "#for ii in range(len(varlist)):\n",
    "varlist=['hand1','hand2','hand3','hand4','hand5','hand6','hand7','hand8','iihandwr','iihandth','iihandsc','iihandto',\n",
    "         'iihandkn','iihandsp','iihandbr','iihandma','iihandbo','iihandfk','iihandey']\n",
    "varslim=['iihandwr','iihandth','iihandsc','iihandto',\n",
    "         'iihandkn','iihandsp','iihandbr','iihandma','iihandbo','iihandfk','iihandey']\n",
    "flagshand=pd.DataFrame(columns=['src_subject_id','site','reason'])\n",
    "for ii in range(len(varslim)):\n",
    "    #varhand=pd.DataFrame(hcdtogether2.loc[hcdtogether2[varlist[ii]]==''])\n",
    "    varhand=pd.DataFrame((hcdtogether2.loc[(hcdtogether2[varslim[ii]].isnull()==True) & (hcdtogether2.hand_total.isnull()==True)]))\n",
    "    varhand['reason']=varslim[ii]+' is missing'\n",
    "    varhandslim=varhand[['src_subject_id','site','reason','interview_age']]\n",
    "    flagshand=flagshand.append(varhandslim,sort=False)\n",
    "\n",
    "flagshand#.reason.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "varlistnoeye=['iihandwr','iihandth','iihandsc','iihandto','iihandkn','iihandsp','iihandbr','iihandma','iihandbo','iihandfk']\n",
    "hcdtogetherolder = hcdtogether2.loc[(hcdtogether2.interview_age.astype(int) >= 132) & (hcdtogether2.src_subject_id != 'HCD0696265')].copy() #subject only came in at V1, so no need to extend logic in this case\n",
    "#assign these older subjects missings to subject's median for score calculation, and then reset them to missing again later\n",
    "#don't forget to reset\n",
    "hcdtogetherolder.loc[(hcdtogetherolder.visit=='V1') & (hcdtogetherolder.src_subject_id=='HCD1807051'),'iihandsp']='4'\n",
    "hcdtogetherolder.loc[(hcdtogetherolder.visit=='V1') & (hcdtogetherolder.src_subject_id=='HCD0036324'),'iihandfk']='5'\n",
    "hcdtogetherolder.loc[(hcdtogetherolder.visit=='V1') & (hcdtogetherolder.src_subject_id=='HCD2415140'),'iihandfk']='4'\n",
    "hcdtogetherolder.loc[(hcdtogetherolder.visit=='V1') & (hcdtogetherolder.src_subject_id=='HCD1502635'),'iihandto']='4'\n",
    "hcdtogetherolder.loc[(hcdtogetherolder.interview_age==227) & (hcdtogetherolder.src_subject_id=='HCD2703751'),'iihandbo']='5'\n",
    "\n",
    "\n",
    "#lightson \n",
    "hcdtogetherolder.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1=hcdtogetherolder.copy()\n",
    "for var in varlistnoeye:\n",
    "    test1[var]=test1.replace({var: {5:100, 4:50, 3:0, 2:-50, 1:-100}})[var]\n",
    "    test1[var]=test1[var].astype(float)\n",
    "    \n",
    "test1['handvarsum']=test1[varlistnoeye].sum(axis=1)\n",
    "test1['handvarsum']=test1['handvarsum']/10\n",
    "test1=test1[varlistnoeye+['src_subject_id','interview_date','handvarsum']]\n",
    "print(test1.columns)\n",
    "\n",
    "test1.to_csv('test1.csv')\n",
    "len(varlistnoeye)\n",
    "\n",
    "test1[varlistnoeye].describe()\n",
    "test1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2=pd.merge(hcdtogetherolder,test1.drop(columns=varlistnoeye),on=['src_subject_id','interview_date'],how='left')\n",
    "test2.handvarsum.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcdtogetherolder=test2.copy()\n",
    "hcdtogetherolder['HCP Handedness Score']=hcdtogetherolder.handvarsum\n",
    "#set the score that are being calculated on missing values to zero\n",
    "hcdtogetherolder.loc[(hcdtogetherolder['HCP Handedness Score'].round(0)==0) & (hcdtogetherolder.hand_total.isnull()==False)&(hcdtogetherolder.iihandwr.isnull()==True)][['hand_total','HCP Handedness Score','iihandwr']]\n",
    "hcdtogetherolder.loc[(hcdtogetherolder['HCP Handedness Score'].round(0)==0)&(hcdtogetherolder.hand_total.isnull()==False)&(hcdtogetherolder.iihandwr.isnull()==True),'HCP Handedness Score']=''\n",
    "hcdtogetherolder.columns\n",
    "\n",
    "#now reset the hand set variables to missing\n",
    "hcdtogetherolder.loc[(hcdtogetherolder.visit=='V1') & (hcdtogetherolder.src_subject_id=='HCD1807051'),'iihandsp']=''\n",
    "hcdtogetherolder.loc[(hcdtogetherolder.visit=='V1') & (hcdtogetherolder.src_subject_id=='HCD0036324'),'iihandfk']=''\n",
    "hcdtogetherolder.loc[(hcdtogetherolder.visit=='V1') & (hcdtogetherolder.src_subject_id=='HCD2415140'),'iihandfk']=''\n",
    "hcdtogetherolder.loc[(hcdtogetherolder.visit=='V1') & (hcdtogetherolder.src_subject_id=='HCD1502635'),'iihandto']=''\n",
    "hcdtogetherolder.loc[(hcdtogetherolder.interview_age==227) & (hcdtogetherolder.src_subject_id=='HCD2703751'),'iihandbo']=''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the scores that are being calculated on missing values to zero\n",
    "superlefts=['HCD2163644','HCD0382343']\n",
    "#hcdtogetherolder.loc[~((hcdtogetherolder.hand_total>0) | ((hcdtogetherolder.src_subject_id.isin(superlefts)) & (hcdtogetherolder.hand6==0.0))),'hand_total']=''#.shape #interview_age.astype(int)>=132,'hand_total']=''\n",
    "#hcdtogetherolder.loc[~((hcdtogetherolder.hand_total>0) | ((hcdtogetherolder.src_subject_id.isin(superlefts)) & (hcdtogetherolder.hand6==0.0)))][['hand_total','hand6','HCP Handedness Score']]\n",
    "hcdtogetherolder.loc[(hcdtogetherolder.hand_total==0) & (~(hcdtogetherolder['HCP Handedness Score']=='') ),'hand_total']=''\n",
    "hcdtogetherolder.loc[(hcdtogetherolder.hand_total==0) & (~(hcdtogetherolder['HCP Handedness Score']=='') ),'hand_total']=''\n",
    "#also set the incorrect 0 score to missing. \n",
    "hcdtogetherolder.loc[(hcdtogetherolder.src_subject_id=='HCD1880162') & (hcdtogetherolder.interview_age==213),'hand_total']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand=hcdtogetherolder[['src_subject_id','gender', 'subjectkey', 'interview_date', 'interview_age','hand1','hand2','hand3','hand4','hand5','hand6','hand7','hand8','hand_total','iihandwr','iihandth','iihandsc',\n",
    "            'iihandto', 'iihandkn','iihandsp', 'iihandbr', 'iihandma','iihandbo','iihandfk','iihandey','HCP Handedness Score']].copy()\n",
    "iihandlist=['iihandwr','iihandth','iihandsc','iihandto', 'iihandkn','iihandsp', 'iihandbr', 'iihandma','iihandbo','iihandfk','iihandey']\n",
    "newhandlist=['newiihandwr','newiihandth','newiihandsc','newiihandto', 'newiihandkn','newiihandsp', 'newiihandbr', 'newiihandma','newiihandbo','newiihandfk','newiihandey']\n",
    "hand[iihandlist].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(iihandlist)):\n",
    "    hand.loc[hand[iihandlist[ii]]=='',iihandlist[ii]]=-999\n",
    "    #print(t.shape)            \n",
    "    hand[iihandlist[ii]]=hand[iihandlist[ii]].fillna(-999).astype(int).round(0).astype(int).astype(str).replace('-999','')\n",
    "    hand[newhandlist[ii]]=hand.replace({iihandlist[ii]:\n",
    "                                       {'1':'left',\n",
    "                                        '2':'spl',\n",
    "                                        '3':'np',\n",
    "                                        '4':'spr',\n",
    "                                        '5':'right'}})[iihandlist[ii]]\n",
    "    \n",
    "handlist=['hand1','hand2','hand3','hand4','hand5','hand6','hand7','hand8']\n",
    "newlist=['newhand1','newhand2','newhand3','newhand4','newhand5','newhand6','newhand7','newhand8']\n",
    "for ii in range(len(handlist)):\n",
    "    t=hand.loc[hand[handlist[ii]]=='']\n",
    "    print(t.shape)            \n",
    "    hand.loc[hand[handlist[ii]]=='',handlist[ii]]=-999\n",
    "    hand[handlist[ii]]=hand[handlist[ii]].fillna(-999).round(0).astype(int).astype(str).replace('-999','')\n",
    "    hand[newlist[ii]]=hand.replace({handlist[ii]:\n",
    "                                       {'1':'right',\n",
    "                                        '0':'left'}})[handlist[ii]]\n",
    "\n",
    "hand.head()\n",
    "hand.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#lightson\n",
    "hand.to_csv('test5.csv',index=False)\n",
    "#print(hand.shape)\n",
    "hand.head()\n",
    "#hand.loc[hand['HCP Handedness Score']==0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hand.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hand.loc[(hand.newhand1.str.strip()=='') | (hand.newhand1.isnull()==True),'newhand1']=hand.newiihandwr\n",
    "hand.loc[(hand.newhand3.str.strip()=='') | (hand.newhand3.isnull()==True),'newhand3']=hand.newiihandth\n",
    "hand.loc[(hand.newhand7.str.strip()=='') | (hand.newhand7.isnull()==True),'newhand7']=hand.newiihandsc\n",
    "hand.loc[(hand.newhand4.str.strip()=='') | (hand.newhand4.isnull()==True),'newhand4']=hand.newiihandto\n",
    "hand.loc[(hand.newhand6.str.strip()=='') | (hand.newhand6.isnull()==True),'newhand6']=hand.newiihandsp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#drop the hands (0s and 1s) and reame the newhands (lefts and rights) back to hands [long story]\n",
    "#drop all the iihandlist variables (values 1::5)\n",
    "#drop newiihandwr, newiihandth, newiihandsc, newiihandto, newiihandsp (values left,spl,np,spr,right) which are are now merged with hand1,hand3,hand7,hand4,hand6, respectively\n",
    "#rename the remaining  newiihandkn, newiihandbr,newiihandma,newiihandbo,newiihandfk,newiihandey back to iihand counterparts\n",
    "\n",
    "hand=hand.drop(columns=iihandlist+handlist+['newiihandwr', 'newiihandth', 'newiihandsc', 'newiihandto', 'newiihandsp'],axis=1)\n",
    "dictnames=dict(zip(newlist,handlist))\n",
    "hand=hand.rename(columns=dictnames)\n",
    "hand=hand.rename(columns={'newiihandkn':'iihandkn','newiihandbr':'iihandbr','newiihandma':'iihandma','newiihandbo':'iihandbo','newiihandfk':'iihandfk','newiihandey':'iihandey'})\n",
    "\n",
    "print(hand.shape)\n",
    "hand.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4/19/2019 change again to element names to avoid confusion of aliases:\n",
    "#rename the collection to original ndar data dictionary elements-note hand5 is a new element...not renamed\n",
    "hand=hand.rename(columns={'hand1':'writing','hand2':'hammer','hand3':'throwing','hand4':'toothbrush','hand6':'spoon','hand7':'scissors','hand8':'hand_15_drink',\n",
    "'iihandkn':'knife_no_fork','iihandbr':'broom','iihandma':'match','iihandbo':'box','iihandfk':'foot','iihandey':'eye'})\n",
    "#hand['hcp_handedness_score']=hand['HCP Handedness Score']\n",
    "hand=hand.rename(columns={'HCP Handedness Score':'hcp_handedness_score'})\n",
    "hand.loc[hand.hcp_handedness_score.isnull()==True]\n",
    "#hand.hcp_handedness_score=hand.hcp_handedness_score.fillna(-9999).astype(int).astype(str).str.replace('-9999','')\n",
    "#print(hand.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hand2=hand.copy()\n",
    "\n",
    "hand2.interview_age=hand.interview_age.astype(int)\n",
    "\n",
    "hand2.loc[hand2.hcp_handedness_score=='','hcp_handedness_score']=-999#.fillna(-999).astype(int).astype(str).replace('-999','')\n",
    "hand2.hcp_handedness_score=hand2.hcp_handedness_score.fillna(-999).astype(int).astype(str).replace('-999','')\n",
    "hand2.interview_date=pd.to_datetime(hand2.interview_date).dt.strftime('%m/%d/%Y')\n",
    "hand2.head()\n",
    "#drop completely empty subject event\n",
    "print(hand2.shape)\n",
    "hand2=hand2.loc[~((hand2.src_subject_id=='HCD1880162') & (hand.interview_age==213))]\n",
    "print(hand2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "filePath=pathout+'/edinburgh_hand01.csv'\n",
    "if os.path.exists(filePath):\n",
    "    os.remove(filePath)\n",
    "else:\n",
    "    print(\"Can not delete the file as it doesn't exists\")\n",
    "\n",
    "with open(filePath,'a') as f:\n",
    "    f.write(\"edinburgh_hand,1\\n\")\n",
    "    hand2.to_csv(f,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NDAsubmissions_venv",
   "language": "python",
   "name": "ndasubmissions_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
