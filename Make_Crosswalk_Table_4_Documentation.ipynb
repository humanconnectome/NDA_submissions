{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will pull all the maps from various sources into one common annotation file\n",
    "Big picture:  you get a folder of HCP behavioral data structures from the NDA and want to know what the elements within them mean.  This notebook takes the list of elements within those structures, and maps them to the NDA documentation about them (e.g. via the data dictionary API) and the local Lifespan documentation about them (to the extent that it is available in REDCap data dictionaries and other human AND machine readable maps.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccf.easy_yaml import EasyYaml\n",
    "from ccf.redcap import RedcapTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import os, datetime\n",
    "import xlrd\n",
    "snapshotdate = datetime.datetime.today().strftime('%m_%d_%Y')\n",
    "rosetta=pd.read_csv('/home/petra/UbWinSharedSpace1/ccf-nda-behavioral/PycharmToolbox/UnrelatedHCAHCD_w_STG_Image_and_pseudo_GUID12_11_2020.csv')\n",
    "rosetta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = EasyYaml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the NDA maps\n",
    "nda = {}\n",
    "for filename in os.listdir('./nda/'):\n",
    "    struct = filename[:-5]\n",
    "    elements = Y('./nda/'+filename)\n",
    "    nda[struct] = elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nda.keys()\n",
    "#nda.values()\n",
    "#nda['asr01'].keys()\n",
    "#nda['asr01']['subjectkey'].keys()\n",
    "#nda['asr01']['subjectkey'].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the redcap data dictionary annotation\n",
    "redcap = {}\n",
    "for filename in os.listdir('./definitions/'):\n",
    "    struct = filename[:-5]\n",
    "    elements = Y('./definitions/'+filename)\n",
    "    redcap[struct] = elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non TOOLBOX prepared dirs\n",
    "nontlbx_hca=\"./Behavioral_HCA726_NonTLBX_20201212 datasetid_ 33780/\"\n",
    "nontlbx_hcd=\"./Behavioral_HCD652_NonTLBX_20210111 dataset id_ 34536/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toolbox Prepared dirs\n",
    "hcapreppedpath='/home/petra/UbWinSharedSpace1/ccf-nda-behavioral/PycharmToolbox/NDA_submissions/NDA_submissions/Behavioral_HCA635_TLBX_20210111 dataset id 34518/'\n",
    "hcdpreppedpath='/home/petra/UbWinSharedSpace1/ccf-nda-behavioral/PycharmToolbox/NDA_submissions/NDA_submissions/Behavioral_HCD488_TLBX_20210111 dataset id 34520/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the variables that are in prepared structures and collect their REDCap and NDA annotation in a single place for a master crosswalk\n",
    "#three main types of files w.r.t annotation : toolbox, moises pipeline, and singletons for each HCA and HCD\n",
    "#singletons break all rules wrt annotation conventions\n",
    "#moises can pull annotation from REDCap or from other Data dictionaries (Penncnp)\n",
    "#get the set of pipeline prepared structures --> point to latest box download to prevent versioning issues\n",
    "\n",
    "#Moises pipeline HCA\n",
    "prepped_hca_elems= []\n",
    "prepped_hca_structs =[]\n",
    "for filename in os.listdir(nontlbx_hca):\n",
    "    if filename[0:4] != 'HCPA':  #singletons require special consideration\n",
    "        struct = filename[:-4]\n",
    "        els = pd.read_csv(nontlbx_hca+filename,header=1).columns.to_list()\n",
    "        struc=[struct]*len(els)\n",
    "        prepped_hca_elems= prepped_hca_elems + els\n",
    "        prepped_hca_structs = prepped_hca_structs + struc\n",
    "    \n",
    "prepped_hca=[1]*len(prepped_hca_structs)\n",
    "print(len(prepped_hca))\n",
    "  \n",
    "adict = {'nda_element':prepped_hca_elems,'nda_structure':prepped_hca_structs,'C-2847 (HCP-A)':prepped_hca}    \n",
    "a=pd.DataFrame(adict)\n",
    "a['collection']='hca'\n",
    "print(a.shape)    \n",
    "\n",
    "\n",
    "#Moises pipeline HCD\n",
    "prepped_hcd_elems= []\n",
    "prepped_hcd_structs =[]\n",
    "for filename in os.listdir(nontlbx_hcd):\n",
    "    if filename[0:4] != 'HCPD':  #singletons require special consideration\n",
    "        struct = filename[:-4]\n",
    "        els = pd.read_csv(nontlbx_hcd+filename,header=1).columns.to_list()\n",
    "        struc=[struct]*len(els)\n",
    "        prepped_hcd_elems= prepped_hcd_elems + els\n",
    "        prepped_hcd_structs = prepped_hcd_structs + struc\n",
    "   \n",
    "prepped_hcd=[1]*len(prepped_hcd_structs)\n",
    "print(len(prepped_hcd))\n",
    "  \n",
    "ddict = {'nda_element':prepped_hcd_elems,'nda_structure':prepped_hcd_structs,'C-2846 (HCP-D)':prepped_hcd}    \n",
    "d=pd.DataFrame(ddict)    \n",
    "d['collection']='hcd'\n",
    "print(d.shape)    \n",
    " \n",
    "#a.head()\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redcap.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip html from redcap annotation\n",
    "import re\n",
    "def html_stripper(text):\n",
    "    if text is None:    \n",
    "        return None\n",
    "    else:\n",
    "        return re.sub('<[^<]+?>', '', text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#capture section nearest previous section header for all variables\n",
    "for db_name, db_elements in redcap.items():\n",
    "    section, form = None, None\n",
    "    for name, element in db_elements.items():\n",
    "        current_form = element.get('form')\n",
    "        current_section = html_stripper(element.get('section'))\n",
    "        if current_form != form:\n",
    "            form = current_form\n",
    "            section = current_section\n",
    "        elif current_section is not None:\n",
    "            section = current_section\n",
    "        if section is not None:    \n",
    "            element['section'] = section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now get the maps (from local sources to nda)\n",
    "db = []\n",
    "for collection in os.listdir('./maps/'):\n",
    "    directory = './maps/' + collection\n",
    "    for filename in os.listdir(directory):\n",
    "        struct = filename[:-5]\n",
    "        elements = Y(os.path.join(directory, filename))['elements']\n",
    "        for e in elements:\n",
    "            e['struct'] = struct\n",
    "            e['collection'] = collection\n",
    "        db.extend(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy #copy function for objects that contain sub-objects\n",
    "\n",
    "#this function will change a list object into a string, where items are separated by '/'\n",
    "def flattened_str(f, x):\n",
    "    n = x.get(f, None)\n",
    "    if type(n) is list:\n",
    "        n = '/'.join(n)\n",
    "    return n\n",
    "\n",
    "#for every row in the 'db' which is a collection of all entries in all yaml maps, elongate, where multiple sources\n",
    "#getting merged into the same element (i.e. so that you can get the correct REDCap annotation from that source)\n",
    "#name and rename (or input and output) should only contain multiple variables if they are coming from the same source\n",
    "#source can be hcpa, child, teen, ssaga, ksads, PennCNP, eprime, parent, qint...\n",
    "#due to the nested nature of sources in the yaml, though, you have to do a little iterating in cases where source is not a single string.\n",
    "#open the bisbas01.yaml map if you need an example.  \n",
    "elongated = []\n",
    "for original in db:\n",
    "    modified = deepcopy(original)    \n",
    "    sources = modified.pop('source')\n",
    "    \n",
    "    if type(sources) is str:\n",
    "        modified[sources] = sources\n",
    "        elongated.append(modified)\n",
    "        \n",
    "    elif type(sources) is list:        \n",
    "        item = deepcopy(modified)\n",
    "        for source in sources:\n",
    "            if type(source) is str:\n",
    "                # e.g., item['parent] = \"parent\"\n",
    "                item[source] = source\n",
    "                elongated.append(item)  \n",
    "                                \n",
    "            elif type(source) is dict:\n",
    "                sourcename, overrides = source.popitem()\n",
    "                newitem = deepcopy(modified)                \n",
    "                newitem[sourcename] = sourcename\n",
    "                newitem.update(overrides)                \n",
    "                elongated.append(newitem)\n",
    "    \n",
    "            elif type(source) is list:\n",
    "                item['collection'] = \"uh-oh\"\n",
    "                elongated.append(item)  \n",
    "                \n",
    "#gonna need the flattened version of name/rename for the next step, but then you can discard this redundant info\n",
    "for original in elongated:\n",
    "    original['output'] = flattened_str('rename', original)\n",
    "    original['input'] = flattened_str('name', original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check - note that 'nda_name' is a field unique to a buggy drugscr01 structure.  If you see it below, \n",
    "#then this issue is still in the process of being addressed.  If you dont see it, then dont worry about it\n",
    "dfdb = pd.DataFrame(elongated)\n",
    "dfdb.columns\n",
    "#dfdb.to_csv('moisesxwalk0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we'll add the NDA and Redcap info, using the 'input' and 'output' variables\n",
    "#remember that not all the annotaiton is actually coming from REDCap, so we'll still have holes to fill.\n",
    "for i in elongated:\n",
    "    # add nda info\n",
    "    if 'output' in i and i['output'] is not None and i['output'].split('/')[0] in nda[i['struct']]:\n",
    "        #only use the first variable (vars are joined with /)\n",
    "        nameo = i['output'].split('/')[0]\n",
    "        x = nda[i['struct']][nameo]\n",
    "        #x = nda[i['struct']][i['output']]\n",
    "        i['type'] = x['type']\n",
    "        i['description'] = x['description'].replace(\"\\r\",\" \")\n",
    "        i['notes'] = x.get('notes')\n",
    "        i['alias'] = x.get('alias')\n",
    "        if 'range' in x:\n",
    "            i['range'] = '; '.join(list(map(str, x['range'])))\n",
    "    \n",
    "    # add redcap info\n",
    "    if 'input' in i and i['input'] is not None:\n",
    "        #only use the first variable (vars are joined with /)\n",
    "        name = i['input'].split('/')[0]\n",
    "        struct = i.get('parent') or i.get('teen') or i.get('child') or i.get('hcpa') or i.get('qint')  or i.get('ssaga') # or i.get('ksads') \n",
    "        if struct is None or name not in redcap[struct]:\n",
    "            continue\n",
    "        x = redcap[struct][name]\n",
    "        i['r_form'] = x.get('form')\n",
    "        i['r_section'] = x.get('section')\n",
    "        i['r_type'] = x.get('type')\n",
    "        i['r_label'] = html_stripper(x.get('label').replace(\"\\n\",\" \").replace(\"\\r\",\" \"))\n",
    "        i['r_choices'] = x.get('choices')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check the columns again\n",
    "#turn into a dataframe\n",
    "df = pd.DataFrame(elongated)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid the duplicate rows (happens when two separate sources have the same transformations - function of source being a list vs a subnested dictionary in the yaml maps)\n",
    "#df.drop_duplicates() #doesnt work because there are lists in the df\n",
    "df=df.loc[df.astype(str).drop_duplicates().index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset source for all of the required variables (incorrectly pulling labels from redcap but all should point to rosetta,\n",
    "#annotation for rosetta will happen later after the singletons and toolbox data annotation are added\n",
    "rosetta_list=['gender','sex','interview_age','interview_date','src_subject_id','subjectkey','family_user_def_id']\n",
    "cols=['PennCNP', 'hcpa', 'qint', 'ssaga', 'child', 'teen', 'parent','name', 'func', 'r_form', 'r_section', 'r_type',\n",
    "       'r_label', 'r_choices', 'request', 'code', 'recode', 'old_code',   \n",
    "       'specialty_code']\n",
    "\n",
    "df['rosetta']=''\n",
    "for i in cols:\n",
    "    df.loc[df['rename'].astype(str).isin(rosetta_list),i]=''\n",
    "    df.loc[df['rename'].astype(str).isin(rosetta_list),'rosetta']='rosetta'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "\n",
    "#remove rows corresponding to ksads data and/or buggy drugscreening for the 2.0 behavioral data release\n",
    "df=df.loc[~(df.ksads=='ksads')]\n",
    "df=df.loc[~(df.struct=='drugscr01')]\n",
    "df=df.drop(columns=['ksads','nda_name'])\n",
    "print(df.shape)\n",
    "df = df.rename(columns={\"PennCNP\": \"penn_cnp\"})\n",
    "\n",
    "\n",
    "#drop columns that were created to make redcap annotation more easy\n",
    "df=df.drop(columns=['input','output'])\n",
    "\n",
    "#reorder and sort so that you can look at the file\n",
    "df=df[['collection','rosetta','penn_cnp', 'hcpa', 'qint', 'ssaga', 'child', 'teen', 'parent','name', 'rename', 'struct',  'type', 'description',\n",
    "       'notes', 'alias', 'range', 'func', 'r_form', 'r_section', 'r_type',\n",
    "       'r_label', 'r_choices', 'request', 'code', 'recode', 'old_code',   \n",
    "       'specialty_code']]\n",
    "\n",
    "#df.sort_values(by=['collection','struct']).to_csv('moisesxwalk.csv',index=False)\n",
    "df.groupby(['collection','struct','rosetta']).count() #just checking that all five rosetta fields are in every structure so far (the family id will be in ndar, but that annotaiton isn't present yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#specialty import of PennCNP annotation, which doesnt exist in a redcap label\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "needsanno=df.loc[(df.penn_cnp==\"PennCNP\")]\n",
    "\n",
    "wb = load_workbook(filename = './DataDictionaries/UPennCNP_Emotion_and_Delayed_Discounting_and_Eprime_mapped_definitions_withEprime_11March2020.xlsx')\n",
    "ws=wb['UPennCNP_Emotion_and_Delated_Di']\n",
    "annot=pd.DataFrame(ws.values)\n",
    "columns=list(annot.iloc[0])#annot.head()\n",
    "annot.columns=columns\n",
    "annot=annot.loc[~(annot.hcp_variable=='hcp_variable')]\n",
    "penn=annot[['hcp_variable','Element Description','test Name','nda_structure']].rename(columns={'hcp_variable':'name','Element Description':'r_label','test Name':'r_form'})\n",
    "penn=penn.loc[penn.nda_structure.isin(['deldisk01','er4001'])]\n",
    "penn=penn[['name','r_label','r_form']]\n",
    "\n",
    "updated = needsanno.merge(penn, how='left', on=['name'], suffixes=('', '_new'))\n",
    "updated['r_label'] = np.where(pd.notnull(updated['r_label_new']), updated['r_label_new'], updated['r_label'])\n",
    "updated['r_form'] = np.where(pd.notnull(updated['r_form_new']), updated['r_form_new'], updated['r_form'])\n",
    "updated.drop(columns=['r_label_new','r_form_new'], axis=1, inplace=True)\n",
    "\n",
    "#put it back together\n",
    "dfA=df.loc[~(df.penn_cnp==\"PennCNP\")]\n",
    "dfnew=pd.concat([dfA,updated],axis=0)\n",
    "dfnew.loc[(dfnew['rename']=='version_form') & (dfnew.struct=='deldisk01'),'name']=\"version_form\"\n",
    "dfnew.loc[(dfnew['rename']=='version_form') & (dfnew.struct=='deldisk01'),'r_label']=\"DELAY_3.5 or PennCNP\"\n",
    "dfnew.loc[dfnew['rename']=='ddisc_valid','r_label']='Current Programming Version of the DDISC_VALID Scoring Code at penncnp.med.upenn.edu'\n",
    "dfnew.loc[dfnew.name=='K_ER40D.valid_code','r_label']='Current Programming Version of the ER40D VALID Scoring Code at penncnp.med.upenn.edu'\n",
    "#dfnew.sort_values(by=['collection','struct']).to_csv('moisesxwalk2.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.columns #looking for merge issues, whereby you have a left and right version of r_forms or something like that\n",
    "#this can happen...especially if you don't start at top of the notebook and proceed all the way to the bottom\n",
    "#dfnew.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW FOR THE SINGLETONS\n",
    "#specialty code to add in ndar_subjects, edinburgh handedness, eprime, facename, singleton structure annotations\n",
    "#FIRST DO HCA\n",
    "singletons=nontlbx_hca\n",
    "#get the set of prepared structures \n",
    "prepped_hca_elems= []\n",
    "prepped_hca_structs =[]\n",
    "prepped_hca_rforms =[]\n",
    "prepped_hca_rsection =[]\n",
    "\n",
    "for filename in os.listdir(singletons):\n",
    "    if 'HCPA' in filename:\n",
    "       print(filename)\n",
    "       els = pd.read_csv(singletons+'/'+filename,header=1).columns.to_list()\n",
    "       if 'facename' in filename:\n",
    "          struc='facename01'\n",
    "          rform='Face Name'\n",
    "          rsection=''\n",
    "       if 'edinburgh' in filename:\n",
    "          struc='edinburgh_hand01'\n",
    "          rform='Intake Interview 2'\n",
    "          rsection='Handedness'\n",
    "       if 'ndar' in filename:\n",
    "          struc='ndar_subject01'\n",
    "          rform=''\n",
    "          rsection=''\n",
    "       struct=[struc]*len(els)\n",
    "       rforms=[rform]*len(els)\n",
    "       rsections=[rsection]*len(els)\n",
    "\n",
    "       prepped_hca_elems= prepped_hca_elems + els\n",
    "       prepped_hca_structs= prepped_hca_structs + struct\n",
    "       prepped_hca_rforms= prepped_hca_rforms + rforms\n",
    "       prepped_hca_rsection= prepped_hca_rsection + rsections\n",
    "       \n",
    "        \n",
    "prepped_hca_elems      \n",
    "prepped_hca_structs\n",
    "singlehca=pd.DataFrame(pd.concat([pd.Series(prepped_hca_elems),pd.Series(prepped_hca_structs)\n",
    "                                  ,pd.Series(prepped_hca_rforms),pd.Series(prepped_hca_rsection)],axis=1))\n",
    "\n",
    "#initializing these vars 'rename' but actually the name/rename designation needs to be finessed by hand, thanks to help-desk\n",
    "#nightmare which initiated 'data harmonization' for the HCP collection\n",
    "#for what its worth:  When study data are downloaded, they come in terms of NDA ELEMENTS, NOT as the ALIAS you used for upload.\n",
    "#ALWAYS KNOW THE NDA ELEMENT to which you are sending the data because NDA does not keep track of study aliases which\n",
    "#can be local or global in scope (also not tracked).  \n",
    "\n",
    "singlehca.columns=['rename','struct','r_form','r_section'] #initializing these vars 'rename' \n",
    "singlehca['hcpa']='hcpa'\n",
    "\n",
    "#the facename vars are coming from both INTRADB And REDCAP.  Our annotation got sent to NDA but not to me (lol), so \n",
    "#I fill in that missing local version of the local annotation later by copying it from the NDA later\n",
    "singlehca['intradb']=''\n",
    "singlehca.loc[singlehca.struct=='facename01','hcpa']=''\n",
    "singlehca.loc[singlehca.struct=='facename01','intradb']='intradb'\n",
    "listr=['f1_recall','f1_other','f2_recall','f2_other','f3_recall','f3_other','f4_recall','f4_other','f5_recall','f5_other','f6_recall','f6_other','f7_recall','f7_other','f8_recall','f8_other','f9_recall','f9_other','f10_recall','f10_other']\n",
    "singlehca.loc[singlehca['rename'].isin(listr),'hcpa']='hcpa'\n",
    "singlehca.loc[singlehca['rename'].isin(listr+rosetta_list),'intradb']=''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check columns\n",
    "singlehca.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#universally assigning name to be the same as rename, but will have to go back and fix so that rename only contains nda_elements, \n",
    "#and name only contains hcp_variables\n",
    "singlehca['name']=singlehca['rename']\n",
    "\n",
    "singlehca.loc[(singlehca.name=='version') & (singlehca.struct=='facename01'),'rename']='version_form'\n",
    "singlehca['collection']='hca'\n",
    "\n",
    "singlehca.loc[singlehca['rename']=='race','name']='racial'\n",
    "singlehca.loc[singlehca['rename']=='ethnic_group','name']='ethnic'\n",
    "singlehca.loc[singlehca.name=='iihandwr','rename']='writing'\n",
    "singlehca.loc[singlehca.name=='iihandth','rename']='throwing'\n",
    "singlehca.loc[singlehca.name=='iihandsc','rename']='scissors'\n",
    "singlehca.loc[singlehca.name=='iihandto','rename']='toothbrush'\n",
    "singlehca.loc[singlehca.name=='iihandkn','rename']='knife_no_fork'\n",
    "singlehca.loc[singlehca.name=='iihandsp','rename']='spoon'\n",
    "singlehca.loc[singlehca.name=='iihandbr','rename']='broom'\n",
    "singlehca.loc[singlehca.name=='iihandma','rename']='match'\n",
    "singlehca.loc[singlehca.name=='iihandbo','rename']='box'\n",
    "singlehca.loc[singlehca.name=='iihandfk','rename']='foot'\n",
    "singlehca.loc[singlehca.name=='iihandey','rename']='eye'\n",
    "\n",
    "\n",
    "#fill out requests as muhc as possible.  reset name to be missing for vars that dont exist locally but were created\n",
    "#to fill a requirement at the NDA\n",
    "singlehca.loc[singlehca.struct.isin(['ndar_subjects','edinburgh_hand01']),'request']='structure created by HCA_ndar_edinburgh_*.ipynb notebook'\n",
    "singlehca.loc[singlehca.struct.isin(['facename01']),'request']='structure created by Stats2Structures.sh '\n",
    "\n",
    "singlehca.loc[singlehca['rename'].isin(['phenotype', 'phenotype_description', 'twins_study',\n",
    "       'sibling_study', 'family_study', 'sample_taken']),'name']=''\n",
    "singlehca.loc[singlehca['rename'].isin(['phenotype', 'phenotype_description', 'twins_study',\n",
    "       'sibling_study', 'family_study', 'sample_taken']),'request']='hardcode required variables'\n",
    "\n",
    "singlehca.loc[singlehca['rename']=='family_user_def_id','name']='final_pedid'\n",
    "#singlehca.to_csv('singletonsHCA.csv')\n",
    "\n",
    "#note will circle back to standardize the rosetta vars after getting HCA singletons together with the HCD singletons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now do HCD\n",
    "singletons=nontlbx_hcd\n",
    "\n",
    "prepped_hcd_elems= []\n",
    "prepped_hcd_structs =[]\n",
    "prepped_hcd_rforms =[]\n",
    "prepped_hcd_rsection =[]\n",
    "\n",
    "for filename in os.listdir(singletons):\n",
    "    if 'HCPD' in filename:\n",
    "       print(filename)\n",
    "       if 'racethnic' not in filename:\n",
    "           els = pd.read_csv(singletons+'/'+filename,header=1).columns.to_list()\n",
    "           if 'edinburgh' in filename:\n",
    "              struc='edinburgh_hand01'\n",
    "              rform='Intake Interview 2'\n",
    "              rsection='Handedness'\n",
    "           if 'ndar' in filename:\n",
    "              struc='ndar_subject01'\n",
    "              rform=''\n",
    "              rsection=''\n",
    "           if 'eprime' in filename:\n",
    "              struc='deldisk01'     \n",
    "              rform='Eprime Delay Discounting'\n",
    "              rsection=''\n",
    "           struct=[struc]*len(els)\n",
    "           rforms=[rform]*len(els)\n",
    "           rsections=[rsection]*len(els)\n",
    "\n",
    "           prepped_hcd_elems= prepped_hcd_elems + els\n",
    "           prepped_hcd_structs= prepped_hcd_structs + struct\n",
    "           prepped_hcd_rforms= prepped_hcd_rforms + rforms\n",
    "           prepped_hcd_rsection= prepped_hcd_rsection + rsections\n",
    "       \n",
    "        \n",
    "prepped_hcd_elems      \n",
    "prepped_hcd_structs\n",
    "singlehcd=pd.DataFrame(pd.concat([pd.Series(prepped_hcd_elems),pd.Series(prepped_hcd_structs)\n",
    "                                  ,pd.Series(prepped_hcd_rforms),pd.Series(prepped_hcd_rsection)],axis=1))\n",
    "singlehcd.columns=['rename','struct','r_form','r_section']\n",
    "singlehcd['collection']='hcd'\n",
    "\n",
    "\n",
    "singlehcd['teen']='teen'\n",
    "singlehcd['child']=''\n",
    "singlehcd['parent']=''\n",
    "singlehcd['eprime']=''\n",
    "#singlehcd.to_csv('singletonsHCD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singlehcd['name']=singlehcd['rename']\n",
    "\n",
    "\n",
    "singlehcd.loc[singlehcd['rename']=='race','name']='sub_race'\n",
    "singlehcd.loc[singlehcd['rename']=='ethnic_group','name']='sub_latino'\n",
    "singlehcd.loc[singlehcd['rename']=='hand_total','child']='child'\n",
    "singlehcd.loc[singlehcd['rename']=='hand_total','teen']=''\n",
    "singlehcd.loc[singlehcd['rename']=='hammer','child']='child'\n",
    "singlehcd.loc[singlehcd['rename']=='hammer','name']='hand2'\n",
    "singlehcd.loc[singlehcd['rename']=='hammer','teen']=''\n",
    "singlehcd.loc[singlehcd['rename']=='hand5','child']='child'\n",
    "singlehcd.loc[singlehcd['rename']=='hand5','teen']=''\n",
    "singlehcd.loc[singlehcd['rename']=='hand_15_drink','child']='child'\n",
    "singlehcd.loc[singlehcd['rename']=='hand_15_drink','teen']=''\n",
    "singlehcd.loc[singlehcd['rename']=='hand_15_drink','name']='hand8'\n",
    "singlehcd.loc[singlehcd['rename']=='writing','name']='iihandwr'\n",
    "singlehcd.loc[singlehcd['rename']=='throwing','name']='iihandth'\n",
    "singlehcd.loc[singlehcd['rename']=='scissors','name']='iihandsc'\n",
    "singlehcd.loc[singlehcd['rename']=='toothbrush','name']='iihandto'\n",
    "singlehcd.loc[singlehcd['rename']=='knife_no_fork','name']='iihandkn'\n",
    "singlehcd.loc[singlehcd['rename']=='spoon','name']='iihandsp'\n",
    "singlehcd.loc[singlehcd['rename']=='broom','name']='iihandbr'\n",
    "singlehcd.loc[singlehcd['rename']=='match','name']='iihandma'\n",
    "singlehcd.loc[singlehcd['rename']=='box','name']='iihandbo'\n",
    "singlehcd.loc[singlehcd['rename']=='foot','name']='iihandfk'\n",
    "singlehcd.loc[singlehcd['rename']=='eye','name']='iihandey'\n",
    "\n",
    "\n",
    "secondrow=singlehcd.loc[singlehcd['rename'].isin(['writing','throwing','toothbrush','spoon','scissors','race','ethnic_group'])].copy()\n",
    "secondrow['r_form']='Intake Interview'\n",
    "secondrow['r_section']='Handedness Assessment'\n",
    "\n",
    "secondrow.loc[secondrow['rename']=='writing','name']='hand1'\n",
    "secondrow.loc[secondrow['rename']=='writing','child']='child'\n",
    "secondrow.loc[secondrow['rename']=='writing','teen']=''\n",
    "secondrow.loc[secondrow['rename']=='throwing','name']='hand3'\n",
    "secondrow.loc[secondrow['rename']=='throwing','child']='child'\n",
    "secondrow.loc[secondrow['rename']=='throwing','teen']=''\n",
    "secondrow.loc[secondrow['rename']=='toothbrush','name']='hand4'\n",
    "secondrow.loc[secondrow['rename']=='toothbrush','child']='child'\n",
    "secondrow.loc[secondrow['rename']=='toothbrush','teen']=''\n",
    "secondrow.loc[secondrow['rename']=='spoon','name']='hand6'\n",
    "secondrow.loc[secondrow['rename']=='spoon','child']='child'\n",
    "secondrow.loc[secondrow['rename']=='spoon','teen']=''\n",
    "secondrow.loc[secondrow['rename']=='scissors','name']='hand7'\n",
    "secondrow.loc[secondrow['rename']=='scissors','child']='child'\n",
    "secondrow.loc[secondrow['rename']=='scissors','teen']=''\n",
    "secondrow.loc[secondrow['rename']=='race','name']='p_c_race'\n",
    "secondrow.loc[secondrow['rename']=='race','child']=''\n",
    "secondrow.loc[secondrow['rename']=='race','teen']=''\n",
    "secondrow.loc[secondrow['rename']=='race','parent']='parent'\n",
    "secondrow.loc[secondrow['rename']=='race','r_form']='Intake Interview'\n",
    "secondrow.loc[secondrow['rename']=='race','r_section']=''\n",
    "secondrow.loc[secondrow['rename']=='ethnic_group','name']='p_c_latino'\n",
    "secondrow.loc[secondrow['rename']=='ethnic_group','child']=''\n",
    "secondrow.loc[secondrow['rename']=='ethnic_group','teen']=''\n",
    "secondrow.loc[secondrow['rename']=='ethnic_group','parent']='parent'\n",
    "secondrow.loc[secondrow['rename']=='ethnic_group','r_form']='Intake Interview'\n",
    "secondrow.loc[secondrow['rename']=='ethnic_group','r_section']=''\n",
    "\n",
    "singlehcd=pd.concat([singlehcd,secondrow],axis=0)\n",
    "\n",
    "#fill out requests as muhc as possible.  reset name to be missing for vars that dont exist locally but were created\n",
    "#to fill a requirement at the NDA\n",
    "singlehcd.loc[singlehcd.struct.isin(['ndar_subjects','edinburgh_hand01']),'request']='structure created by HCD_ndar_edinburgh_*.ipynb notebook'\n",
    "singlehcd.loc[singlehcd.struct.isin(['deldisk01']),'request']='structure created by HCD_Eprime_deldisk01_*.ipynb notebook'\n",
    "singlehcd.loc[singlehcd['rename']=='family_user_def_id','name']='final_pedid'\n",
    "singlehcd.loc[singlehcd['rename']=='hand_total','rename']='handedness_score'\n",
    "\n",
    "singlehcd.loc[singlehcd['rename'].isin(['phenotype', 'phenotype_description', 'twins_study',\n",
    "       'sibling_study', 'family_study', 'sample_taken']),'name']=''\n",
    "singlehcd.loc[singlehcd['rename'].isin(['phenotype', 'phenotype_description', 'twins_study',\n",
    "       'sibling_study', 'family_study', 'sample_taken']),'request']='hardcode required variables'\n",
    "\n",
    "singlehcd.loc[singlehcd.struct=='deldisk01','teen']=''\n",
    "singlehcd.loc[singlehcd.struct=='deldisk01', 'eprime']='eprime'\n",
    "\n",
    "rosetta_list=['gender','sex','interview_age','interview_date','src_subject_id','subjectkey','family_user_def_id']\n",
    "singlehcd.loc[singlehcd['rename'].astype(str).isin(rosetta_list),'eprime']=''\n",
    "#singlehcd.to_csv('singletonsHCD.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the singletons together for the redcap annotation grab\n",
    "singletonsHCP=pd.concat([singlehca,singlehcd],axis=0)\n",
    "#it used to be the case that 'gender' was the nda_element name for sex.  NDA has since changed this, though\n",
    "#gender and sex can both be used\n",
    "#HCP used 'gender' to refer to sex at birth, too.  Yay.  \n",
    "singletonsHCP.loc[singletonsHCP['rename']=='gender','rename']='sex'\n",
    "#streamline the rosetta vars and make sure there are 5 for each structure (except ndar, which will have the family var, too)\n",
    "\n",
    "#reset source for all of the required variables (incorrectly pulling labels from redcap but all should point to rosetta,\n",
    "#annotation for rosetta will happen later after the singletons and toolbox data annotation are added\n",
    "rosetta_list=['gender','sex','interview_age','interview_date','src_subject_id','subjectkey','family_user_def_id']\n",
    "cols=['penn_cnp', 'hcpa', 'qint', 'ssaga', 'child', 'teen', 'parent','name', 'func', 'r_form', 'r_section', 'r_type',\n",
    "       'r_label', 'r_choices', 'request', 'code', 'recode', 'old_code',   \n",
    "       'specialty_code']\n",
    "\n",
    "singletonsHCP['rosetta']=''\n",
    "for i in cols:\n",
    "    singletonsHCP.loc[singletonsHCP['rename'].astype(str).isin(rosetta_list),i]=''\n",
    "    singletonsHCP.loc[singletonsHCP['rename'].astype(str).isin(rosetta_list),'rosetta']='rosetta'\n",
    "\n",
    "\n",
    "singletonsHCP.head()\n",
    "#singletonsHCP.to_csv('tttt.csv')\n",
    "#check columns\n",
    "singletonsHCP.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now pull in any available redcap annotation if source is hcpa child teen or parent\n",
    "redcaphcpa=pd.DataFrame(redcap['hcpa']).transpose()\n",
    "redcaphcpa=redcaphcpa.reset_index().rename(columns={'index':'name','label':'r_label','type':'r_type','choices':'r_choices'})[['name','r_type','r_label','r_choices']]\n",
    "redcaphcpa['hcpa']='hcpa'\n",
    "singletonsHCPa=pd.merge(singletonsHCP.loc[singletonsHCP.hcpa=='hcpa'].drop(columns=['r_type','r_label','r_choices']),redcaphcpa,how='left',on=['hcpa','name'])\n",
    "\n",
    "redcapchild=pd.DataFrame(redcap['child']).transpose()\n",
    "redcapchild=redcapchild.reset_index().rename(columns={'index':'name','label':'r_label','type':'r_type','choices':'r_choices'})[['name','r_type','r_label','r_choices']]\n",
    "redcapchild['child']='child'\n",
    "singletonsHCPb=pd.merge(singletonsHCP.loc[singletonsHCP.child=='child'].drop(columns=['r_type','r_label','r_choices']),redcapchild,how='left',on=['child','name'])\n",
    "\n",
    "redcapteen=pd.DataFrame(redcap['teen']).transpose()\n",
    "redcapteen=redcapteen.reset_index().rename(columns={'index':'name','label':'r_label','type':'r_type','choices':'r_choices'})[['name','r_type','r_label','r_choices']]\n",
    "redcapteen['teen']='teen'\n",
    "singletonsHCPc=pd.merge(singletonsHCP.loc[singletonsHCP.teen=='teen'].drop(columns=['r_type','r_label','r_choices']),redcapteen,how='left',on=['teen','name'])\n",
    "\n",
    "redcapparent=pd.DataFrame(redcap['parent']).transpose()\n",
    "redcapparent=redcapparent.reset_index().rename(columns={'index':'name','label':'r_label','type':'r_type','choices':'r_choices'})[['name','r_type','r_label','r_choices']]\n",
    "redcapparent['parent']='parent'\n",
    "singletonsHCPd=pd.merge(singletonsHCP.loc[singletonsHCP.parent=='parent'].drop(columns=['r_type','r_label','r_choices']),redcapparent,how='left',on=['parent','name'])\n",
    "\n",
    "singletonsHCPe=singletonsHCP.loc[(singletonsHCP.eprime=='eprime') | (singletonsHCP.intradb=='intradb')  | (singletonsHCP.rosetta=='rosetta')]\n",
    "#redcapanno=pd.concat([redcaphcpa,redcapchild,redcapteen,redcapparent],axis=0)\n",
    "#singletonsHCP=pd.merge(singletonsHCP,redcapanno,how='left',on=['parent','hcpa','teen','child','name'])\n",
    "\n",
    "print(singletonsHCP.shape)\n",
    "singletonsHCPtest=pd.concat([singletonsHCPa,singletonsHCPb,singletonsHCPc,singletonsHCPd,singletonsHCPe],axis=0)\n",
    "print(singletonsHCPtest.shape)\n",
    "\n",
    "singletonsHCPtest.columns                                                                                                           \n",
    "#singletonsHCPtest.to_csv('ssss.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nda1=pd.DataFrame(nda['edinburgh_hand01']).transpose().reset_index().rename(columns={'index':'rename'})[['type','rename','description','notes','range','alias']]\n",
    "nda1['struct']='edinburgh_hand01'\n",
    "nda2=pd.DataFrame(nda['facename01']).transpose().reset_index().rename(columns={'index':'rename'})[['type','rename','description','notes','range','alias']]\n",
    "nda2['struct']='facename01'\n",
    "nda3=pd.DataFrame(nda['ndar_subject01']).transpose().reset_index().rename(columns={'index':'rename'})[['type','rename','description','notes','range','alias']]\n",
    "nda3['struct']='ndar_subject01'\n",
    "nda4=pd.DataFrame(nda['deldisk01']).transpose().reset_index().rename(columns={'index':'rename'})[['type','rename','description','notes','range','alias']]\n",
    "nda4['struct']='deldisk01'\n",
    "nda_anno=pd.concat([nda1,nda2,nda3,nda4],axis=0)\n",
    "nda_anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singletonsHCPtest.columns\n",
    "singletonsHCPtest2=pd.merge(singletonsHCPtest,nda_anno,how='left',on=['struct','rename'])\n",
    "singletonsHCPtest2.columns\n",
    "singletonsHCPtest2.to_csv('uuuu.csv')\n",
    "\n",
    "\n",
    "#copy the NDA facename var descriptions over to the local annotation \n",
    "#r_form is missing for all the rosetta vars, so you can use it as a pull indicator\n",
    "singletonsHCPtest2.loc[singletonsHCPtest2.r_form=='Face Name','r_label']=singletonsHCPtest2.description\n",
    "\n",
    "#copy the NDA var descriptions for a few other special cases;\n",
    "singletonsHCPtest2.loc[singletonsHCPtest2.name=='hcp_handedness_score','r_label']=singletonsHCPtest2.description\n",
    "\n",
    "#now get the eprime special annotation\n",
    "singletonsHCPtest2.loc[singletonsHCPtest2.name.str.contains('ddisc'),'r_label']=singletonsHCPtest2.description\n",
    "\n",
    "singletonsHCPtest2.loc[(singletonsHCPtest2.name=='version_form') & (singletonsHCPtest2.struct=='deldisk01'),'r_label']=\"DELAY_3.5 or PennCNP\"\n",
    "singletonsHCPtest2.loc[(singletonsHCPtest2.name=='version_form') & (singletonsHCPtest2.struct=='deldisk01'),'code']=\"return 'DELAY_3.5'\"\n",
    "\n",
    "singletonsHCPtest2.loc[singletonsHCPtest2['rename']=='comqother','name']=''\n",
    "singletonsHCPtest2.loc[singletonsHCPtest2['rename']=='comqother','code']=\"return 'subject about self'\"\n",
    "#singletonsHCPtest2.to_csv('uuuu.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now concatenate these two sources of annotation (moises and singletons) and do a little cleanup\n",
    "#print(singletonsHCPtest2.columns)\n",
    "#print(dfnew.columns)\n",
    "\n",
    "NonTLBX=pd.concat([singletonsHCPtest2,dfnew],axis=0)\n",
    "#dfnew.sort_values(by=['collection','struct']).to_csv('moisesxwalk2.csv',index=False)\n",
    "NonTLBX.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beautify column names\n",
    "NonTLBX = NonTLBX.rename(columns={\n",
    "        'name':'hcp_variable_name', \n",
    "        'rename': 'nda_element',\n",
    "        'struct':'nda_structure',\n",
    "        'request':'nda_request',\n",
    "         #func,code,recode\n",
    "        #'hcpa':'REDCap7 HCPA',\n",
    "        'type':'nda_type',\n",
    "        'description':'nda_description', \n",
    "        'notes':'nda_notes',\n",
    "        'alias':'nda_aliases',\n",
    "        'range':'nda_range',\n",
    "        'r_label':'hcp_label',\n",
    "        'r_choices':'choices_calcs'\n",
    "         #'penn_cnp': 'Box Curated PennCNP',\n",
    "        #'qint': 'REDCap9 Qinteractive', \n",
    "        #'ksads': 'REDCap9 KSADs',\n",
    "        #'teen':'REDCap7 HCPD-18', \n",
    "        #'child':'REDCap7 HCPD-child',\n",
    "        #'parent':'REDCap7 HCPD-Parent', \n",
    "        #'ssaga':'REDCap7 HCPA-SSAGA'\n",
    "   })\n",
    "NonTLBX.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do some concatenating and rosetta filling to make the crosswalk readable\n",
    "NonTLBX['source']=NonTLBX['rosetta'].fillna('') + NonTLBX['hcpa'].fillna('') + NonTLBX['intradb'].fillna('') + NonTLBX['ssaga'].fillna('') +  NonTLBX['penn_cnp'].fillna('') + NonTLBX['qint'].fillna('')+ NonTLBX['teen'].fillna('') +  NonTLBX['child'].fillna('') +  NonTLBX['parent'].fillna('') +  NonTLBX['eprime'].fillna('') \n",
    "NonTLBX.r_section=NonTLBX.r_section.str.replace('<[^<]+?>', '')\n",
    "NonTLBX['hcp_instrument']=NonTLBX['r_form'].fillna('') +\":\"+ NonTLBX['r_section'].fillna('')\n",
    "NonTLBX['hcp_instrument']=NonTLBX['hcp_instrument'].str.strip(\":\")\n",
    "\n",
    "\n",
    "#create hcp_variable_names and labels and Instruments for Rosetta vars\n",
    "rosetta_list=['sex','interview_age','interview_date','src_subject_id','subjectkey','family_user_def_id']\n",
    "rosetta_local=['nda_gender','nda_interview_age','nda_interview_date','subjectped','nda_guid','final_pedid']\n",
    "NonTLBX.loc[NonTLBX['nda_element']=='sex','hcp_variable_name']='nda_gender'\n",
    "NonTLBX.loc[NonTLBX['nda_element']=='interview_age','hcp_variable_name']='nda_interview_age'\n",
    "NonTLBX.loc[NonTLBX['nda_element']=='interview_date','hcp_variable_name']='nda_interview_date'\n",
    "NonTLBX.loc[NonTLBX['nda_element']=='src_subject_id','hcp_variable_name']='subjectped'\n",
    "NonTLBX.loc[NonTLBX['nda_element']=='subjectkey','hcp_variable_name']= 'nda_guid'\n",
    "NonTLBX.loc[NonTLBX['nda_element']=='family_user_def_id','hcp_variable_name']='final_pedid'\n",
    "\n",
    "NonTLBX.loc[NonTLBX['nda_element']=='sex','hcp_label']='sex at birth'\n",
    "NonTLBX.loc[NonTLBX['nda_element']=='interview_age','hcp_label']='age in months'\n",
    "NonTLBX.loc[NonTLBX['nda_element']=='interview_date','hcp_label']='RedCap event registration date when copied to IntraDB (rounded down to nearest Quarter)'\n",
    "NonTLBX.loc[NonTLBX['nda_element']=='src_subject_id','hcp_label']='HCA or HCD subject id'\n",
    "NonTLBX.loc[NonTLBX['nda_element']=='subjectkey','hcp_label']='Pseudo-Guid' \n",
    "NonTLBX.loc[NonTLBX['nda_element']=='family_user_def_id','hcp_label']='family identifier for related subjects within and across HCA and HCD studies'\n",
    "\n",
    "NonTLBX.loc[NonTLBX.rosetta=='rosetta','hcp_instrument']='UnrelatedHCAHCD_w_STG_Image_and_pseudo_GUID12_11_2020.csv'\n",
    "\n",
    "NonTLBX.loc[NonTLBX.nda_element=='comqother','hcp_variable_name']='dummy'\n",
    "NonTLBX.loc[NonTLBX.nda_element=='comqother','hcp_label']='Respondent (subject AND object) for stacked vs wide structure schema clarification'\n",
    "NonTLBX.loc[(NonTLBX.nda_element=='respond') & (NonTLBX.nda_structure=='eatq01'),'hcp_variable_name']='dummy'\n",
    "NonTLBX.loc[(NonTLBX.nda_element=='respond') & (NonTLBX.nda_structure=='pds01'),'hcp_variable_name']='dummy'\n",
    "NonTLBX.loc[(NonTLBX.nda_element=='respond') & (NonTLBX.nda_structure=='srs02'),'hcp_variable_name']='dummy'\n",
    "\n",
    "NonTLBX.loc[(NonTLBX.nda_element=='version_form') & (NonTLBX.nda_structure=='bisbas01'),'hcp_label']='dummy variable: there is no version'\n",
    "NonTLBX.loc[(NonTLBX.nda_element=='version_form') & (NonTLBX.nda_structure=='bisbas01'),'hcp_variable_name']='dummy'\n",
    "NonTLBX.loc[(NonTLBX.nda_element=='version_form') & (NonTLBX.nda_structure=='cbcl01'),'hcp_variable_name']='dummy'\n",
    "NonTLBX.loc[(NonTLBX.nda_element=='version_form') & (NonTLBX.nda_structure=='gbi01'),'hcp_variable_name']='dummy'\n",
    "NonTLBX.loc[(NonTLBX.nda_element=='version_form') & (NonTLBX.nda_structure=='mctq01'),'hcp_variable_name']='dummy'\n",
    "NonTLBX.loc[(NonTLBX.nda_element=='version_form') & (NonTLBX.nda_structure=='medh01'),'hcp_variable_name']='dummy'\n",
    "NonTLBX.loc[(NonTLBX.nda_element=='version_form') & (NonTLBX.nda_structure=='phenx_su01'),'hcp_variable_name']='dummy'\n",
    "NonTLBX.loc[(NonTLBX.nda_element=='version_form') & (NonTLBX.nda_structure=='srs02'),'hcp_variable_name']='dummy'\n",
    "NonTLBX.loc[(NonTLBX.nda_element=='version_form') & (NonTLBX.nda_structure=='ysr01'),'hcp_variable_name']='dummy'\n",
    "NonTLBX.loc[(NonTLBX.nda_element=='version_form') & (NonTLBX.nda_structure=='upps01'),'hcp_variable_name']='dummy'\n",
    "NonTLBX.loc[(NonTLBX.nda_element=='versionchildadult') & (NonTLBX.nda_structure=='trail_ca01'),'hcp_variable_name']='dummy'\n",
    "\n",
    "moredummies=['cbcl_activities','cbcl_activities_raw','cbcl_adhd_raw','cbcl_affective','cbcl_affective_raw','cbcl_affective_raw','cbcl_anxiety_raw','cbcl_depresspr','cbcl_depresspr_raw','cbcl_emotional','cbcl_emotional_raw','cbcl_emotional_raw','cbcl_ocd','cbcl_ocd_raw','cbcl_oppositional_raw','cbcl_pervasive','cbcl_pervasive_raw','cbcl_pervasive_raw','cbcl_ptsd','cbcl_ptsd_raw','cbcl_school','cbcl_school_raw','cbcl_sct','cbcl_sct_raw','cbcl_sleep','cbcl_sleep_raw','cbcl_social_c','cbcl_social_c_raw','cbcl_total_c','cbcl_total_c_raw','cbcl_withdrawn_raw','pds_boy_rs','pds_girl_rs','phenotype','ravlt_delt','ravlt_disct','ravlt_tott','respond_detail','seizures','sports_time1','sports_time2','sports_time3','sports_well1','sports_well2','sports_well3','phenotype','phenotype_description','family_study','twins_study','sibling_study','sample_taken']\n",
    "\n",
    "for i in moredummies:\n",
    "    newi='dummy'+i\n",
    "    #print(newi)\n",
    "    NonTLBX.loc[NonTLBX.nda_element==i,'hcp_variable_name']='dummy'\n",
    "\n",
    "    \n",
    "\n",
    "NonTLBX['nda_structure_link']=\"https://nda.nih.gov/data_structure.html?short_name=\"+NonTLBX['nda_structure']\n",
    "NonTLBX=NonTLBX[['collection','rosetta', 'hcpa',  'intradb','ssaga', 'penn_cnp', 'qint', \n",
    "                 'teen',   'child', 'parent', 'eprime', 'source',\n",
    "                 'nda_element', 'nda_structure', 'nda_type', 'nda_description', 'nda_notes',\n",
    "       'nda_range', 'nda_aliases',  'nda_structure_link', 'nda_request','hcp_variable_name', 'hcp_label', \n",
    "        'r_type','hcp_instrument', 'r_form', 'r_section', 'choices_calcs', 'func', 'code', 'recode', 'old_code', 'specialty_code']]\n",
    "NonTLBX=NonTLBX[['collection','source',\n",
    "                 'nda_element', 'nda_structure', 'nda_description', 'nda_notes',\n",
    "       'nda_range',  'nda_structure_link','nda_request', 'hcp_variable_name', 'hcp_label', \n",
    "          'hcp_instrument', 'choices_calcs', 'func', 'code', 'recode']]\n",
    "\n",
    "NonTLBX.loc[NonTLBX.source=='teenparent','source']='teen/parent'#concatenate form and section\n",
    "NonTLBX.loc[NonTLBX.source=='teenchild','source']='teen/child'#concatenate form and section\n",
    "NonTLBX.loc[NonTLBX.source=='childparent','source']='child/parent'#concatenate form and section\n",
    "NonTLBX.loc[NonTLBX.source=='teenchildparent','source']='teen/child/parent'#concatenate form and section\n",
    "\n",
    "\n",
    "#remove any remaining html from the redcap fields\n",
    "NonTLBX.hcp_label=NonTLBX.hcp_label.str.replace('<[^<]+?>', '')\n",
    "\n",
    "#for the dummy vars in ndar_subjects, give hcp_label the ndar_label\n",
    "NonTLBX.columns\n",
    "#ndlist=['phenotype','phenotype_description','family_study','twins_study','sibling_study','sample_taken']\n",
    "NonTLBX.loc[(NonTLBX.nda_structure=='ndar_subject01') & (NonTLBX.nda_element=='phenotype'),'hcp_label']='Phenotype/diagnosis for the subject'\n",
    "NonTLBX.loc[(NonTLBX.nda_structure=='ndar_subject01') & (NonTLBX.nda_element=='phenotype_description'),'hcp_label']='Description of the phenotype for the subject'\n",
    "NonTLBX.loc[(NonTLBX.nda_structure=='ndar_subject01') & (NonTLBX.nda_element=='family_study'),'hcp_label']='Was it family study? Study of biological mother, biological father and/or sibling of proband.'\n",
    "NonTLBX.loc[(NonTLBX.nda_structure=='ndar_subject01') & (NonTLBX.nda_element=='twins_study'),'hcp_label']='Is this study of twins?'\n",
    "NonTLBX.loc[(NonTLBX.nda_structure=='ndar_subject01') & (NonTLBX.nda_element=='sibling_study'),'hcp_label']='Was it sibling study? Study of sibling(s) of autistic child.'\n",
    "NonTLBX.loc[(NonTLBX.nda_structure=='ndar_subject01') & (NonTLBX.nda_element=='sample_taken'),'hcp_label']='Was a sample taken at this interview/during this project time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NonTLBX.loc[(NonTLBX.hcp_variable_name=='dummy') & (NonTLBX.hcp_label.isnull()==True) & (NonTLBX.code=='return 999'),'hcp_label']='dummy var required but na'\n",
    "NonTLBX.loc[(NonTLBX.hcp_variable_name=='dummy') & (NonTLBX.hcp_label.isnull()==True) & (NonTLBX.code=='return -98'),'hcp_label']='dummy var required but na'\n",
    "NonTLBX.loc[(NonTLBX.hcp_variable_name=='dummy') & (NonTLBX.hcp_label.isnull()==True) & (NonTLBX.code.str.contains('return')),'hcp_label']='dummy var - see return in trans_code or trans_nda_request'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NonTLBX.loc[NonTLBX.hcp_label.isnull()==True]\n",
    "#NonTLBX.loc[(NonTLBX.nda_structure=='cbcl01') & (NonTLBX.hcp_variable_name=='dummy') & (NonTLBX.code=='return 999'),'hcp_label']#='dummy var required but missing'\n",
    "#NonTLBX.loc[(NonTLBX.nda_structure=='cbcl1_501') & (NonTLBX.hcp_variable_name=='dummy') & (NonTLBX.code=='return 999'),'hcp_label']#='dummy var required but missing'\n",
    "#NonTLBX.loc[(NonTLBX.nda_structure=='ds01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NonTLBX.sort_values(by=['collection','nda_structure']).to_csv('Crosswalk_HCP_NonTLBX_'+snapshotdate+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now load the TLBX annotation and reconfigure so it can be concatenated with the rest of the stuff\n",
    "#get collection, source, and nda_elements from the prepared structures, then merge it with the loaded Toolbox Crosswalk\n",
    "#drop existing NDA annot and pull in fresh stuff\n",
    "crosswalkpath=\"/home/petra/UbWinSharedSpace1/ccf-nda-behavioral/PycharmToolbox/Ipad2NDA_withCrosswalk/NIHToolbox2NDA/\"\n",
    "cfile=\"Crosswalk_NIH_Toolbox_2_NDA.csv\"\n",
    "crosswalk=pd.read_csv(crosswalkpath+cfile,header=0,low_memory=False, encoding = \"ISO-8859-1\")\n",
    "crosswalk=crosswalk.drop(columns=['Measurement System','validated','DataType','specialty_code','template','inst_short','Source','description','valueRange','notes'])\n",
    "\n",
    "crosswalk=crosswalk.rename(columns={'Responses':'choices_calcs','Inst':'hcp_instrument','hcp_variable':'hcp_variable_name',\n",
    "                          'action_requested':'nda_request','requested_python':'func','description':'nda_description','valueRange':'nda_range','notes':'nda_notes'})\n",
    "\n",
    "crosswalk['hcp_label']=crosswalk['Domain'].fillna('') +\":\"+ crosswalk['Item ID'].fillna('') +\":\"+ crosswalk['Stem'].fillna('') +\":\"+ crosswalk['Context'].fillna('')\n",
    "crosswalk['hcp_label']=crosswalk['hcp_label'].str.strip(\":\")\n",
    "crosswalk.loc[crosswalk.hcp_label.fillna('')=='','hcp_label']=crosswalk.hcp_variable_name \n",
    "crosswalk=crosswalk.drop(columns=['Domain','Item ID','Stem','Context'])\n",
    "                                  \n",
    "crosswalk['source']='NIH Toolbox'\n",
    "print(crosswalk.shape)\n",
    "crosswalk=crosswalk.loc[crosswalk.nda_element.isnull()==False]\n",
    "print(crosswalk.shape)\n",
    "crosswalk.columns\n",
    "\n",
    "#crosswalk.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of instruments: will need to merge into output, \n",
    "instruments=list(crosswalk.hcp_instrument.unique())\n",
    "#filename='HCPA_Anger-Affect_prang01_12_12_2020.csv'\n",
    "#row = pd.read_csv(hcapreppedpath+filename,header=1, low_memory=False,nrows=1)\n",
    "#inst=row.version_form[0]\n",
    "#print(row.version_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TLBX pipeline uploaded variables (should have been nda_elements...they were not.  were fixed.  check again now...\n",
    "#look for empty descriptions \n",
    "\n",
    "prepped_hca_elems= []\n",
    "prepped_hca_structs =[]\n",
    "prepped_hca_instruments =[]\n",
    "\n",
    "for filename in os.listdir(hcapreppedpath):\n",
    "#    print(filename)\n",
    "    struct = pd.read_csv(hcapreppedpath+filename,header=None, low_memory=False,nrows=1)\n",
    "    structure=str((struct[0]+'0'+struct[1].astype(str))[0])\n",
    "    if structure=='cogcomp01':\n",
    "        inst='Cognition Composite Scores'\n",
    "    else:\n",
    "        row = pd.read_csv(hcapreppedpath+filename,header=1, low_memory=False,nrows=1)\n",
    "        inst=row.version_form[0]\n",
    "    els = pd.read_csv(hcapreppedpath+filename,header=1).columns.to_list()\n",
    "    instrument=[inst]*len(els)\n",
    "    struc=[structure]*len(els)\n",
    "    prepped_hca_elems= prepped_hca_elems + els\n",
    "    prepped_hca_structs = prepped_hca_structs + struc \n",
    "    prepped_hca_instruments=prepped_hca_instruments + instrument\n",
    "    \n",
    "#print(len(prepped_hca))\n",
    "  \n",
    "adict = {'hcp_variable_upload':prepped_hca_elems,'nda_structure':prepped_hca_structs,'hcp_instrument':prepped_hca_instruments}    \n",
    "a=pd.DataFrame(adict)\n",
    "a['collection']='hca'\n",
    "print(a.shape)    \n",
    "#a.head()\n",
    "\n",
    "prepped_hcd_elems= []\n",
    "prepped_hcd_structs =[]\n",
    "prepped_hcd_instruments =[]\n",
    "\n",
    "for filename in os.listdir(hcdpreppedpath):\n",
    "    struct = pd.read_csv(hcdpreppedpath+filename,header=None, low_memory=False,nrows=1)\n",
    "    structure=str((struct[0]+'0'+struct[1].astype(str))[0])\n",
    "    if structure=='cogcomp01':\n",
    "        inst='Cognition Composite Scores'\n",
    "    else:\n",
    "        row = pd.read_csv(hcdpreppedpath+filename,header=1, low_memory=False,nrows=1)\n",
    "        inst=row.version_form[0]\n",
    "    els = pd.read_csv(hcdpreppedpath+filename,header=1).columns.to_list()\n",
    "    instrument=[inst]*len(els)\n",
    "    struc=[structure]*len(els)\n",
    "    prepped_hcd_elems= prepped_hcd_elems + els\n",
    "    prepped_hcd_structs = prepped_hcd_structs + struc\n",
    "    prepped_hcd_instruments=prepped_hcd_instruments + instrument\n",
    "    \n",
    "#print(len(prepped_hcd))\n",
    "  \n",
    "ddict = {'hcp_variable_upload':prepped_hcd_elems,'nda_structure':prepped_hcd_structs,'hcp_instrument':prepped_hcd_instruments}    \n",
    "d=pd.DataFrame(ddict)\n",
    "d['collection']='hcd'\n",
    "print(d.shape)    \n",
    "d.head()\n",
    "\n",
    "tlbxstructs=pd.concat([a,d],axis=0)\n",
    "#tlbxstructs.shape\n",
    "#tlbxstructs.loc[tlbxstructs.nda_element=='gender','nda_element']='sex'\n",
    "tlbxstructs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlbxstructs.columns\n",
    "crosswalk.columns\n",
    "#merge by hcp_instrument and hcp_variable_upload...check to see everything is there\n",
    "uploadwcross=pd.merge(tlbxstructs,crosswalk.drop(columns='nda_structure'),how='left',on=['hcp_instrument','hcp_variable_upload'])\n",
    "#uploadwcross.to_csv('uuuu.csv')  #whew - only the rosetta elements are missing plus 3 from the tlbx_empbeh01\n",
    "#fill in missing nda_element for next merge\n",
    "\n",
    "uploadwcross.loc[uploadwcross.hcp_variable_upload=='subjectkey','nda_element']='subjectkey'\n",
    "uploadwcross.loc[uploadwcross.hcp_variable_upload=='src_subject_id','nda_element']='src_subject_id'\n",
    "uploadwcross.loc[uploadwcross.hcp_variable_upload=='interview_age','nda_element']='interview_age'\n",
    "uploadwcross.loc[uploadwcross.hcp_variable_upload=='interview_date','nda_element']='interview_date'\n",
    "uploadwcross.loc[uploadwcross.hcp_variable_upload=='gender','nda_element']='sex'\n",
    "\n",
    "#fill in the blanks\n",
    "uploadwcross['nda_structure_link']='https://nda.nih.gov/data_structure.html?short_name='+uploadwcross.nda_structure\n",
    "uploadwcross['source']='NIH Toolbox Ipad App'\n",
    "#uploadwcross.to_csv('uuuu.csv')\n",
    "uploadwcross.columns\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploadwcross.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now get the updated annotation\n",
    "\n",
    "import requests\n",
    "import json\n",
    "#get nda annotation for all the elements uploaded\n",
    "structs=list(tlbxstructs.nda_structure.unique())\n",
    "#for a given structure (shortname), grab all the metadata for a list of elements as a dataframe\n",
    "def getNDAdetails(structure_name='ndar_subject01',crosswalk=tlbxstructs):\n",
    "    varlist=list(crosswalk.loc[crosswalk.nda_structure==structure_name,'nda_element'].unique())\n",
    "    r = requests.get('https://ndar.nih.gov/api/datadictionary/datastructure/{}'\n",
    "                 .format(structure_name),\n",
    "                  headers={'Accept':'application/json'})\n",
    "    structure = json.loads(r.text)\n",
    "    df=pd.DataFrame(structure['dataElements'])\n",
    "    df2=df[['name','description','valueRange','notes','aliases','type']].copy()\n",
    "    dfxwalk=df2.loc[df2.name.isin(varlist)].copy() \n",
    "    dfxwalk['nda_structure']=structure_name\n",
    "    dfxwalk=dfxwalk.rename(columns={'name':'nda_element','description':'nda_description','notes':'nda_notes','type':'nda_type','valueRange':'nda_range'})\n",
    "    #type, aliases\n",
    "    return dfxwalk\n",
    "\n",
    "nda_anno=pd.DataFrame()\n",
    "for i in structs:\n",
    "    dfstruct=getNDAdetails(structure_name=i,crosswalk=uploadwcross)\n",
    "    nda_anno=pd.concat([nda_anno,dfstruct],axis=0)\n",
    "    \n",
    "\n",
    "print(nda_anno.shape)\n",
    "nda_anno.head()\n",
    "nda_anno.loc[nda_anno.nda_element=='dccsmixed_shape_repeat14']\n",
    "#now merge this annotation with the tlbxstructs (lots of repeats due to stacking of vars via 'inst' : i.e. more structs uploaded than will be downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uploadwcross.shape)\n",
    "TLBX=pd.merge(uploadwcross,nda_anno,on=['nda_element','nda_structure'],how='left')\n",
    "print(TLBX.shape)\n",
    "#TLBX.to_csv('test3.csv',index=False)\n",
    "list(TLBX.nda_structure.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix rosetta var annotation\n",
    "rosetta_list=['gender','sex','interview_age','interview_date','src_subject_id','subjectkey','family_user_def_id']\n",
    "cols=['source', 'func', 'hcp_label', 'hcp_instrument','choices_calcs', 'nda_request']\n",
    "\n",
    "for i in cols:\n",
    "    TLBX.loc[TLBX['nda_element'].astype(str).isin(rosetta_list),i]=''\n",
    "    TLBX.loc[TLBX['nda_element'].astype(str).isin(rosetta_list),'source']='rosetta'\n",
    "\n",
    "TLBX.loc[TLBX.nda_element=='gender','nda_element']='sex'\n",
    "\n",
    "\n",
    "needsanno=TLBX.loc[TLBX.nda_element.isin(['sex','interview_age','interview_date','src_subject_id','subjectkey'])]\n",
    "drops=['nda_description','nda_notes','nda_range','nda_structure_link','nda_request','hcp_variable_name','hcp_label','hcp_instrument']\n",
    "newanno=pd.read_csv('./DataDictionaries/rosetta_anno.csv')\n",
    "\n",
    "needsanno=pd.merge(needsanno.drop(columns=drops),newanno,how='left',on=['nda_element'])\n",
    "needsanno.shape #drop_duplicates()  \n",
    "needsanno.to_csv('testit.csv')\n",
    "hasanno=TLBX.loc[~(TLBX.nda_element.isin(['sex','interview_age','interview_date','src_subject_id','subjectkey']))]\n",
    "\n",
    "TLBXbetter=pd.concat([needsanno,hasanno],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "TLBXbetter.to_csv('test.csv')\n",
    "TLBXbetter.shape\n",
    "TLBXbetter2=TLBXbetter[['collection','source',\n",
    "                 'nda_element', 'nda_structure', 'nda_description', 'nda_notes',\n",
    "       'nda_range',  'nda_structure_link','nda_request', 'hcp_variable_name', 'hcp_label', \n",
    "          'hcp_instrument', 'choices_calcs', 'func']]\n",
    "Crosswalk=pd.concat([TLBXbetter2,NonTLBX], axis=0)[['collection',\n",
    "                 'nda_element', 'nda_structure', 'nda_description', 'nda_notes',\n",
    "       'nda_range',   'hcp_variable_name', 'hcp_label', \n",
    "          'hcp_instrument', 'source','choices_calcs', 'nda_request', 'func', 'code', 'recode','nda_structure_link']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Crosswalk.loc[Crosswalk.collection=='hca','collection']='C-2847 (HCP-A)'\n",
    "Crosswalk.loc[Crosswalk.collection=='hcd','collection']='C-2846 (HCP-D)'\n",
    "Crosswalk=Crosswalk.rename(columns={'func':'trans_func','code':'trans_code','recode':'trans_recode'})\n",
    "Crosswalk.loc[Crosswalk.hcp_variable_name=='height','hcp_label']=\"Height: ft'in'\"\n",
    "Crosswalk.loc[Crosswalk.hcp_variable_name=='weight','hcp_label']=\"Weight: LBS\"\n",
    "Crosswalk.sort_values(by=['collection','nda_structure']).to_csv('Crosswalk_Lifespan_Behavioral_2.0_'+snapshotdate+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#structure_var_stats=df.groupby(['C-2847 (HCP-A)', 'C-2846 (HCP-D)','nda_structure']).count()[['hcp_variable_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_var_stats=structure_var_stats.rename(columns={'hcp_variable_name':'number of HCP variables'})\n",
    "structure_var_stats.to_csv('Collection_by_Structure_'+snapshotdate+'.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NDA_submissions_venv",
   "language": "python",
   "name": "nda_submissions_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
